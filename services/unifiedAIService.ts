import { GoogleGenAI, GenerateContentResponse, Part, Content, Chat, Type } from "@google/genai";
import { ChatMessage, Conversation, GeminiModel, insightTabsConfig, PlayerProfile, GameContext, EnhancedInsightTab, DetectedTask, TaskCompletionPrompt, Insight } from "./types";
import { profileService } from "./profileService";
import { playerProfileService } from "./playerProfileService";
import { aiContextService } from "./aiContextService";
import { characterDetectionService } from "./characterDetectionService";
import { unifiedUsageService } from "./unifiedUsageService";
import { authService } from "./supabase";
import { apiCostService } from "./apiCostService";
import { feedbackLearningEngine } from "./feedbackLearningEngine";
import { supabaseDataService } from './supabaseDataService';
import { progressTrackingService } from './progressTrackingService';
import { longTermMemoryService } from './longTermMemoryService';
import { screenshotTimelineService } from './screenshotTimelineService';
import { ServiceFactory, BaseService } from './ServiceFactory';
import { STORAGE_KEYS } from '../utils/constants';
// Static imports to replace dynamic imports for Firebase hosting compatibility
import { progressiveInsightService } from './progressiveInsightService';
import { otakuDiaryService } from './otakuDiaryService';
import { taskCompletionPromptingService } from './taskCompletionPromptingService';
import { structuredResponseService } from './structuredResponseService';
// Additional imports from geminiService for enhanced functionality
import { dailyNewsCacheService } from './dailyNewsCacheService';
import { universalContentCacheService, type CacheQuery } from './universalContentCacheService';

/**
Â * ğŸ¯ UNIFIED AI SERVICE
Â *Â 
Â * This service consolidates all AI and insight functionality from:
Â * - geminiService.ts (AI interactions)
Â * - enhancedInsightService.ts (Enhanced insights)
Â * - proactiveInsightService.ts (Proactive insights)
Â * - profileAwareInsightService.ts (Profile-aware insights)
Â * - suggestedPromptsService.ts (Suggested prompts)
Â * - insightService.ts (Basic insights)
Â *Â 
Â * Features:
Â * 1. Unified AI interactions with Gemini
Â * 2. Intelligent insight generation
Â * 3. Profile-aware recommendations
Â * 4. Proactive insight suggestions
Â * 5. Smart prompt suggestions
Â * 6. Cost optimization strategies
Â * 7. Context-aware responses
Â */

// ===== AI SERVICE INTERFACES =====

export interface AIResponse {
  content: string;
  suggestions: string[];
  gameInfo?: {
    gameId: string;
    confidence: 'high' | 'low';
    progress?: number;
    genre?: string;
  };
  metadata: {
    model: GeminiModel;
    timestamp: number;
    cost: number;
    tokens: number;
  };
  suggestedTasks?: DetectedTask[]; // NEW: AI suggested tasks
  taskCompletionPrompt?: TaskCompletionPrompt; // NEW: Task completion prompt
}

// ===== UNIVERSAL AI RESPONSE INTERFACE (1:1 API Call Architecture) =====

export interface UniversalAIResponse {
  // The main chat response for the user
  narrativeResponse: string;
  
  // AI-suggested tasks (replaces secondary API call)
  suggestedTasks: DetectedTask[];
  
  // Progressive insight updates (replaces background call)
  progressiveInsightUpdates: {
    tabId: string;
    title: string;
    content: string;
  }[];
  
  // Game state changes detected from user query
  stateUpdateTags: string[];
  
  // Follow-up prompts for user engagement
  followUpPrompts: string[];
  
  // Game pill creation data (for Pro/Vanguard users)
  gamePillData?: {
    shouldCreate: boolean;
    gameName: string;
    genre: string;
    wikiContent: Record<string, string>;
  };
  
  // Task completion prompt data
  taskCompletionPrompt?: TaskCompletionPrompt;
  
  // Metadata for tracking
  metadata: {
    model: string;
    tokens: number;
    cost: number;
    timestamp: number;
  };
}

// NEW: Interface for detected tasks (moved to types.ts to break circular dependency)
// export interface DetectedTask {
//Â  Â title: string;
//Â  Â description: string;
//Â  Â category: 'quest' | 'boss' | 'exploration' | 'item' | 'character' | 'custom';
//Â  Â confidence: number;
//Â  Â source: string;
// }

export interface InsightResult {
Â  tabId: string;
Â  title: string;
Â  content: string;
Â  priority: 'high' | 'medium' | 'low';
Â  isProfileSpecific: boolean;
Â  generationModel: 'flash' | 'pro';
Â  lastUpdated: number;
Â  category: 'enhanced' | 'proactive' | 'profile_aware' | 'basic';
}

export interface ProactiveTrigger {
Â  type: 'objective_complete' | 'inventory_change' | 'area_discovery' |Â 
Â  Â  Â  Â  'session_start' | 'session_end' | 'progress_milestone' |Â 
Â  Â  Â  Â  'difficulty_spike' | 'exploration_pattern';
Â  gameId: string;
Â  gameTitle: string;
Â  data: Record<string, any>;
Â  timestamp: number;
}

export interface ProactiveInsightSuggestion {
Â  id: string;
Â  title: string;
Â  content: string;
Â  priority: 'high' | 'medium' | 'low';
Â  triggerType: ProactiveTrigger['type'];
Â  gameId: string;
Â  timestamp: number;
Â  metadata: Record<string, any>;
}

export interface PromptSuggestion {
Â  id: string;
Â  text: string;
Â  category: 'general' | 'game_specific' | 'contextual' | 'follow_up';
Â  priority: number;
Â  used: boolean;
Â  metadata: Record<string, any>;
}

export interface AIConfig {
Â  useProactiveInsights: boolean;
Â  useProfileAwareInsights: boolean;
Â  useEnhancedInsights: boolean;
Â  costOptimization: boolean;
Â  maxSuggestions: number;
Â  insightCacheEnabled: boolean;
}

// ===== UNIFIED AI SERVICE =====

// Constants from geminiService
const COOLDOWN_KEY = 'geminiCooldownEnd';
const NEWS_CACHE_KEY = 'otakonNewsCache';

// Helper functions from geminiService
const isQuotaError = (error: any): boolean => {
Â  const errorMessage = error.toString();
Â  const httpStatus = error.httpError?.status;
Â  return errorMessage.includes("RESOURCE_EXHAUSTED") || httpStatus === 429;
};

export class UnifiedAIService extends BaseService {
  private ai!: GoogleGenAI;
  private chatSessions: Record<string, { chat: Chat, model: GeminiModel }> = {};
  private config: AIConfig;
  private usedPrompts: Set<string> = new Set();
  private insightCache: Map<string, InsightResult[]> = new Map();
  private readonly COOLDOWN_DURATION = 60 * 60 * 1000; // 1 hour
  
  // âœ… MEMORY LEAK FIXES: Track resources for cleanup
  private intervals = new Set<NodeJS.Timeout>();
  private abortControllers = new Set<AbortController>();
  private eventListeners = new Map<string, () => void>();

Â  constructor() {
Â  Â  super();
Â  Â Â 
Â  Â  const API_KEY = process.env.API_KEY;
Â  Â  if (!API_KEY) {
Â  Â  Â  console.warn("Gemini API Key not found. Please set the API_KEY environment variable.");
Â  Â  }
Â  Â Â 
Â  Â  // Don't initialize AI immediately to avoid API key errors during static import
Â  Â  // AI will be initialized lazily when first needed
Â  Â  this.config = {
Â  Â  Â  useProactiveInsights: true,
Â  Â  Â  useProfileAwareInsights: true,
Â  Â  Â  useEnhancedInsights: true,
Â  Â  Â  costOptimization: true,
Â  Â  Â  maxSuggestions: 4,
Â  Â  Â  insightCacheEnabled: true
Â  Â  };
Â  Â Â 
Â  Â  this.initialize();
Â  }

Â  private ensureAIInitialized(): void {
Â  Â  if (!this.ai) {
Â      const API_KEY = (import.meta as any)?.env?.VITE_GEMINI_API_KEY || process.env.API_KEY;
Â  Â  Â Â 
Â  Â  Â  // Debug API key loading (only in development)
      if ((import.meta as any)?.env?.DEV) {
Â  Â  Â  Â  console.log('ğŸ”§ AI Service Debug:', {
          viteKey: (import.meta as any)?.env?.VITE_GEMINI_API_KEY ? 'âœ… Set' : 'âŒ Missing',
Â  Â  Â  Â  Â  processKey: process.env.API_KEY ? 'âœ… Set' : 'âŒ Missing',
Â  Â  Â  Â  Â  finalKey: API_KEY ? 'âœ… Set' : 'âŒ Missing'
Â  Â  Â  Â  });
Â  Â  Â  }
Â  Â  Â Â 
Â  Â  Â  if (!API_KEY) {
Â  Â  Â  Â  console.warn("Gemini API Key not found. Please set the VITE_GEMINI_API_KEY environment variable.");
Â  Â  Â  Â  return;
Â  Â  Â  }
Â  Â  Â  this.ai = new GoogleGenAI({ apiKey: API_KEY });
Â  Â  }
Â  }

Â  // ===== INITIALIZATION =====

Â  private initialize(): void {
Â  Â  this.loadUsedPrompts();
Â  Â  this.loadInsightCache();
Â  Â  console.log('âœ… UnifiedAIService initialized successfully');
Â  }

Â  // ===== CORE AI METHODS =====

Â  async generateResponse(
Â  Â  conversation: Conversation,
Â  Â  message: string,
Â  Â  hasImages: boolean = false,
Â  Â  signal?: AbortSignal,
Â  Â  conversationHistory: ChatMessage[] = []
Â  ): Promise<AIResponse & { progressiveUpdates?: Record<string, { title: string; content: string }> }> {
Â  Â  try {
Â  Â  Â  // Check cooldown
Â  Â  Â  if (await this.checkCooldown()) {
Â  Â  Â  Â  throw new Error('AI service is on cooldown. Please try again later.');
Â  Â  Â  }

Â  Â  Â  // NEW: Initialize long-term memory for this conversation
Â  Â  Â  await longTermMemoryService.initializeLongTermSession(conversation.id, conversation.id);
Â  Â  Â Â 
Â  Â  Â  // NEW: Track this interaction
Â  Â  Â  await longTermMemoryService.trackInteraction(conversation.id, 'message', { message });

Â  Â  Â  // Get optimal model
Â  Â  Â  const model = this.getOptimalModel('chat');
Â  Â  Â Â 
Â  Â  Â  // NEW: Get system instruction with long-term memory context
Â  Â  Â  const systemInstruction = await this.getLongTermAwareSystemInstruction(conversation, hasImages);
Â  Â  Â Â 
Â  Â  Â  // Prepare content
Â  Â  Â  const content = this.prepareContent(message, hasImages);
Â  Â  Â Â 
Â  Â  Â  // Generate response
Â  Â  Â  const response = await this.generateContent({
Â  Â  Â  Â  model,
Â  Â  Â  Â  contents: content,
Â  Â  Â  Â  config: { systemInstruction },
Â  Â  Â  Â  signal
Â  Â  Â  });

Â  Â  Â  // Process response
Â  Â  Â  const processedResponse = await this.processResponse(response, model);
Â  Â  Â Â 
Â  Â  Â  // NEW: Track response for long-term memory
Â  Â  Â  await longTermMemoryService.trackInteraction(conversation.id, 'insight', {
Â  Â  Â  Â  type: 'ai_response',
Â  Â  Â  Â  content: processedResponse.content,
Â  Â  Â  Â  relevance: 1.0
Â  Â  Â  });
Â  Â  Â Â 
Â  Â  Â  // Update usage tracking
Â  Â  Â  await this.updateUsageTracking(processedResponse.metadata);

Â  Â  Â  // Generate progressive insight updates in the background (non-blocking)
Â  Â  Â  let progressiveUpdates: Record<string, { title: string; content: string }> = {};
Â  Â  Â  if (conversation.id !== 'everything-else' && conversation.insights) {
Â  Â  Â  Â  try {
Â  Â  Â  Â  Â  // Using static import instead of dynamic import for Firebase hosting compatibility
Â  Â  Â  Â  Â  const progressiveContext = {
Â  Â  Â  Â  Â  Â  gameName: conversation.title || 'Unknown Game',
Â  Â  Â  Â  Â  Â  genre: conversation.genre || 'default',
Â  Â  Â  Â  Â  Â  progress: conversation.progress || 0,
Â  Â  Â  Â  Â  Â  userQuery: message,
Â  Â  Â  Â  Â  Â  aiResponse: processedResponse.content,
Â  Â  Â  Â  Â  Â  conversationHistory,
Â  Â  Â  Â  Â  Â  currentInsightTabs: conversation.insights
Â  Â  Â  Â  Â  };

Â  Â  Â  Â  Â  // Run progressive updates in background (don't await to avoid blocking response)
Â  Â  Â  Â  Â  progressiveInsightService.updateInsightTabsProgressively(progressiveContext, signal)
Â  Â  Â  Â  Â  Â  .then(result => {
Â  Â  Â  Â  Â  Â  Â  if (Object.keys(result.updatedTabs).length > 0) {
Â  Â  Â  Â  Â  Â  Â  Â  console.log('ğŸ¯ Progressive insight updates generated:', result.relevantTabIds);
Â  Â  Â  Â  Â  Â  Â  Â  // Store updates for later retrieval
Â  Â  Â  Â  Â  Â  Â  Â  this.storeProgressiveUpdates(conversation.id, result.updatedTabs);
Â  Â  Â  Â  Â  Â  Â  }
Â  Â  Â  Â  Â  Â  })
Â  Â  Â  Â  Â  Â  .catch(error => {
Â  Â  Â  Â  Â  Â  Â  console.warn('Progressive insight updates failed:', error);
Â  Â  Â  Â  Â  Â  });
Â  Â  Â  Â  } catch (error) {
Â  Â  Â  Â  Â  console.warn('Failed to initiate progressive insight updates:', error);
Â  Â  Â  Â  }
Â  Â  Â  }

Â  Â  Â  // NEW: Generate AI suggested tasks for Pro/Vanguard users
Â  Â  Â  let suggestedTasks: DetectedTask[] = [];
Â  Â  Â  if (conversation.id !== 'everything-else') {
Â  Â  Â  Â  try {
Â  Â  Â  Â  Â  const userTier = await unifiedUsageService.getTier();
Â  Â  Â  Â  Â  if (userTier === 'pro' || userTier === 'vanguard_pro') {
Â  Â  Â  Â  Â  Â  suggestedTasks = await this.generateSuggestedTasks(
Â  Â  Â  Â  Â  Â  Â  conversation,
Â  Â  Â  Â  Â  Â  Â  message,
Â  Â  Â  Â  Â  Â  Â  processedResponse.content,
Â  Â  Â  Â  Â  Â  Â  signal
Â  Â  Â  Â  Â  Â  );
Â  Â  Â  Â  Â  Â  console.log(`ğŸ¯ Generated ${suggestedTasks.length} AI suggested tasks for ${conversation.title}`);
Â  Â  Â  Â  Â  }
Â  Â  Â  Â  } catch (error) {
Â  Â  Â  Â  Â  console.warn('Failed to generate suggested tasks:', error);
Â  Â  Â  Â  }
Â  Â  Â  }

Â  Â  Â  // NEW: Generate task completion prompt
Â  Â  Â  let taskCompletionPrompt: TaskCompletionPrompt | undefined;
Â  Â  Â  if (conversation.id !== 'everything-else') {
Â  Â  Â  Â  try {
Â  Â  Â  Â  Â  const userTier = await unifiedUsageService.getTier();
Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  // Get central tasks (user-created + AI-generated tasks they added)
Â  Â  Â  Â  Â  const centralTasks = await otakuDiaryService.getCentralTasks(conversation.id);
Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  // Get AI-generated tasks (for Pro/Vanguard users when no central tasks)
Â  Â  Â  Â  Â  const aiGeneratedTasks = await otakuDiaryService.getAISuggestedTasks(conversation.id);
Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  // Generate task completion prompt using static import
Â  Â  Â  Â  Â  taskCompletionPrompt = taskCompletionPromptingService.generateCompletionPrompt(
Â  Â  Â  Â  Â  Â  conversation.id,
Â  Â  Â  Â  Â  Â  userTier,
Â  Â  Â  Â  Â  Â  centralTasks,
Â  Â  Â  Â  Â  Â  aiGeneratedTasks
Â  Â  Â  Â  Â  ) || undefined;
Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  if (taskCompletionPrompt) {
Â  Â  Â  Â  Â  Â  console.log(`ğŸ“ Generated task completion prompt for ${conversation.title}: ${taskCompletionPrompt.tasks.length} tasks`);
Â  Â  Â  Â  Â  }
Â  Â  Â  Â  } catch (error) {
Â  Â  Â  Â  Â  console.warn('Failed to generate task completion prompt:', error);
Â  Â  Â  Â  }
Â  Â  Â  }
Â  Â  Â Â 
      return { ...processedResponse, progressiveUpdates, suggestedTasks, taskCompletionPrompt };
    } catch (error) {
      console.error('Failed to generate AI response:', error);
      throw error;
    }
  }

  // ===== NEW: UNIVERSAL RESPONSE METHOD (1:1 API Call Architecture) =====

  /**
   * Generate a comprehensive AI response using a single API call
   * This method consolidates all AI functionality into one call to achieve 1:1 API call ratio
   */
  async generateUniversalResponse(
    conversation: Conversation,
    message: string,
    hasImages: boolean = false,
    signal?: AbortSignal,
    conversationHistory: ChatMessage[] = []
  ): Promise<UniversalAIResponse> {
    try {
      // Check if this is one of the 4 suggested prompts that should use cached responses
      const suggestedPrompts = [
        "What's the latest gaming news?",
        "Which games are releasing soon?", 
        "What are the latest game reviews?",
        "Show me the hottest new game trailers."
      ];
      
      const isSuggestedPrompt = suggestedPrompts.some(prompt => 
        message.toLowerCase().includes(prompt.toLowerCase())
      );
      
      if (isSuggestedPrompt) {
        console.log('ğŸ“° Detected suggested prompt, checking cache...');
        
        // Import the daily news cache service
        const { dailyNewsCacheService } = await import('./dailyNewsCacheService');
        
        // Check for cached response
        const cachedResponse = dailyNewsCacheService.getCachedResponse(message);
        if (cachedResponse) {
          console.log('ğŸ“° Serving cached response for suggested prompt');
          return {
            narrativeResponse: cachedResponse.content,
            suggestedTasks: [],
            progressiveInsightUpdates: [],
            stateUpdateTags: [],
            followUpPrompts: [
              "What's the latest gaming news?",
              "Which games are releasing soon?",
              "What are the latest game reviews?",
              "Show me the hottest new game trailers."
            ],
            gamePillData: null,
            taskCompletionPrompt: null,
            metadata: {
              model: 'gemini-2.5-flash',
              tokens: 0,
              cost: 0,
              timestamp: Date.now()
            }
          };
        }
        
        // If no cache, check if we can trigger grounding search
        const userTier = await unifiedUsageService.getTier();
        const searchCheck = await dailyNewsCacheService.needsGroundingSearch(message, userTier);
        
        if (!searchCheck.needsSearch) {
          console.log('ğŸ“° Cannot trigger grounding search:', searchCheck.reason);
          return {
            narrativeResponse: `I'd love to help you with that! However, ${searchCheck.reason.toLowerCase()}. Please try again later or consider upgrading to Pro/Vanguard for more frequent updates.`,
            suggestedTasks: [],
            progressiveInsightUpdates: [],
            stateUpdateTags: [],
            followUpPrompts: [
              "What's the latest gaming news?",
              "Which games are releasing soon?",
              "What are the latest game reviews?",
              "Show me the hottest new game trailers."
            ],
            gamePillData: null,
            taskCompletionPrompt: null,
            metadata: {
              model: 'gemini-2.5-flash',
              tokens: 0,
              cost: 0,
              timestamp: Date.now()
            }
          };
        }
        
        console.log('ğŸ“° Triggering grounding search for suggested prompt - will use grounding search in AI call');
      }

      // Check cooldown
      if (await this.checkCooldown()) {
        throw new Error('AI service is on cooldown. Please try again later.');
      }

      // Initialize long-term memory for this conversation
      await longTermMemoryService.initializeLongTermSession(conversation.id, conversation.id);
      
      // Track this interaction
      await longTermMemoryService.trackInteraction(conversation.id, 'message', { message });

      // Get optimal model
      const model = this.getOptimalModel('chat');
      
      // Get system instruction with long-term memory context
      let systemInstruction = await this.getUniversalSystemInstruction(conversation, hasImages, message, conversationHistory);
      
      // Add special instructions for suggested prompts with grounding search
      if (isSuggestedPrompt) {
        systemInstruction += `

**CRITICAL: SUGGESTED PROMPT WITH GROUNDING SEARCH**
You are responding to one of the 4 suggested prompts that requires real-time gaming news:
- "What's the latest gaming news?"
- "Which games are releasing soon?"
- "What are the latest game reviews?"
- "Show me the hottest new game trailers."

**MANDATORY REQUIREMENTS:**
1. **USE GROUNDING SEARCH**: You have access to Google Search. Use it to find the most recent, accurate gaming news.
2. **REAL-TIME DATA**: Focus on news from the last few days/weeks, not outdated information.
3. **COMPREHENSIVE RESPONSE**: Provide detailed, specific information about recent gaming developments.
4. **PROPER FORMATTING**: Use headers, bullet points, and clear sections for readability.
5. **CURRENT EVENTS**: Include specific game announcements, release dates, reviews, trailers, etc.

**RESPONSE STRUCTURE:**
- Start with a brief overview of the current gaming landscape
- Provide specific, recent news items with details
- Include relevant links or references when possible
- End with follow-up suggestions

**DO NOT:**
- Give generic responses about "gaming world buzzing"
- Use outdated information
- Provide vague answers
- Skip the grounding search - it's essential for accuracy`;
      }
      
      // Add structured formatting instructions based on player profile
      try {
        const playerProfile = await playerProfileService.getProfile();
        const gameContext = conversation.genre ? await playerProfileService.getGameContext(conversation.title || '') : null;

        if (playerProfile && gameContext) {
          const intent = structuredResponseService.analyzeUserIntent(
            message,
            conversationHistory.map(msg => msg.text),
            '', // lastGameId
            conversation.id
          );
          
          const formatting = structuredResponseService.generateResponseStructure(intent, playerProfile, gameContext);
          const formattingInstructions = structuredResponseService.generateFormattingInstructions(formatting, intent, playerProfile);
          
          // Append formatting instructions to system instruction
          systemInstruction += '\n\n' + formattingInstructions;
          
          console.log(`ğŸ¨ Applied structured formatting for ${intent} intent with ${playerProfile.hintStyle} style`);
        }
      } catch (error) {
        console.warn('Failed to apply structured formatting, using default:', error);
      }
      
      // Prepare content
      const content = this.prepareContent(message, hasImages);
      
      // Add grounding search tools for suggested prompts
      let tools: any[] = [];
      if (isSuggestedPrompt) {
        console.log('ğŸ“° Adding grounding search tools for suggested prompt');
        tools = [
          {
            googleSearchRetrieval: {
              dynamicRetrievalConfig: {
                mode: "MODE_DYNAMIC",
                dynamicThreshold: 0.7
              }
            }
          }
        ];
      }
      
      // Generate comprehensive response using single API call
      const response = await this.generateContent({
        model,
        contents: content,
        config: { 
          systemInstruction,
          responseMimeType: "application/json",
          tools: tools,
          responseSchema: {
            type: Type.OBJECT,
            properties: {
              narrativeResponse: { type: Type.STRING },
              suggestedTasks: {
                type: Type.ARRAY,
                items: {
                  type: Type.OBJECT,
                  properties: {
                    title: { type: Type.STRING },
                    description: { type: Type.STRING },
                    category: { type: Type.STRING },
                    confidence: { type: Type.NUMBER },
                    source: { type: Type.STRING }
                  }
                }
              },
              progressiveInsightUpdates: {
                type: Type.ARRAY,
                items: {
                  type: Type.OBJECT,
                  properties: {
                    tabId: { type: Type.STRING },
                    title: { type: Type.STRING },
                    content: { type: Type.STRING }
                  }
                }
              },
              stateUpdateTags: { type: Type.ARRAY, items: { type: Type.STRING }},
              followUpPrompts: { type: Type.ARRAY, items: { type: Type.STRING }},
              gamePillData: {
                type: Type.OBJECT,
                properties: {
                  shouldCreate: { type: Type.BOOLEAN },
                  gameName: { type: Type.STRING },
                  genre: { type: Type.STRING },
                  wikiContent: { 
                    type: Type.OBJECT,
                    properties: {
                      tabId: { type: Type.STRING },
                      content: { type: Type.STRING }
                    }
                  }
                }
              },
              taskCompletionPrompt: {
                type: Type.OBJECT,
                properties: {
                  tasks: { 
                    type: Type.ARRAY, 
                    items: { 
                      type: Type.OBJECT,
                      properties: {
                        id: { type: Type.STRING },
                        title: { type: Type.STRING },
                        description: { type: Type.STRING },
                        category: { type: Type.STRING },
                        status: { type: Type.STRING }
                      }
                    }
                  },
                  promptText: { type: Type.STRING },
                  category: { type: Type.STRING }
                }
              }
            },
            required: ["narrativeResponse", "suggestedTasks", "progressiveInsightUpdates", "stateUpdateTags", "followUpPrompts"]
          }
        }
      });

      // Parse the JSON response
      if (!response) {
        throw new Error("AI returned an empty response.");
      }
      
      // Parse JSON with robust error handling
      const universalResponse = this.parseUniversalResponseSafely(response, conversation.id, message.length, hasImages);
      if (!universalResponse) {
        throw new Error('Failed to parse universal response JSON');
      }

      // Add metadata
      universalResponse.metadata = {
        model: model as GeminiModel,
        tokens: 0, // Token count not available from string response
        cost: 0, // Cost calculation not available without token count
        timestamp: Date.now()
      };

      // Cache response for suggested prompts
      if (isSuggestedPrompt) {
        try {
          const { dailyNewsCacheService } = await import('./dailyNewsCacheService');
          const userTier = await unifiedUsageService.getTier();
          const userId = await authService.getCurrentUserId();
          
          await dailyNewsCacheService.cacheFreshResponse(
            message,
            universalResponse.narrativeResponse,
            userTier,
            userId
          );
          
          console.log('ğŸ“° Cached fresh response for suggested prompt');
        } catch (error) {
          console.warn('Failed to cache suggested prompt response:', error);
        }
      }

      // Track response for long-term memory
      await longTermMemoryService.trackInteraction(conversation.id, 'insight', {
        type: 'ai_response',
        content: universalResponse.narrativeResponse,
        relevance: 1.0
      });
      
      // Update usage tracking
      await this.updateUsageTracking({
        model: universalResponse.metadata.model as GeminiModel,
        tokens: universalResponse.metadata.tokens,
        cost: universalResponse.metadata.cost,
        timestamp: universalResponse.metadata.timestamp
      });

      // Handle progressive insight updates
      if (universalResponse.progressiveInsightUpdates.length > 0) {
        this.storeProgressiveUpdates(conversation.id, 
          universalResponse.progressiveInsightUpdates.reduce((acc, update) => {
            acc[update.tabId] = { title: update.title, content: update.content };
            return acc;
          }, {} as Record<string, { title: string; content: string }>)
        );
      }

      // Handle game pill creation for all users
      if (universalResponse.gamePillData?.shouldCreate) {
        console.log('ğŸ® [GamePill] AI requested game pill creation:', {
          gameName: universalResponse.gamePillData.gameName,
          genre: universalResponse.gamePillData.genre,
          conversationId: conversation.id,
          tabCount: Object.keys(universalResponse.gamePillData.wikiContent || {}).length,
          userTier: await unifiedUsageService.getTier()
        });
        await this.handleGamePillCreation(conversation, universalResponse.gamePillData, signal);
      } else if (universalResponse.gamePillData) {
        console.log('ğŸš« [GamePill] AI decided NOT to create game pill:', {
          shouldCreate: universalResponse.gamePillData.shouldCreate,
          conversationId: conversation.id,
          reason: conversation.id !== 'everything-else' ? 'Already in game-specific conversation' : 'AI decision'
        });
      } else {
        console.log('ğŸ” [GamePill] No game pill data in AI response for conversation:', conversation.id);
      }

      return universalResponse;
    } catch (error) {
      console.error('Universal Response System Debug:', {
        errorType: (error as Error).constructor.name,
        errorMessage: (error as Error).message,
        conversationId: conversation.id,
        hasImages,
        messageLength: message.length,
        conversationTitle: conversation.title,
        historyLength: conversationHistory.length
      });
      
      // Check for specific error types
      if ((error as Error).message.includes('cooldown')) {
        console.error('Universal response failed due to cooldown');
      } else if ((error as Error).message.includes('quota')) {
        console.error('Universal response failed due to quota exceeded');
      } else if ((error as Error).message.includes('JSON')) {
        console.error('Universal response failed due to JSON parsing');
      } else if ((error as Error).message.includes('abort')) {
        console.error('Universal response was aborted by user');
      } else {
        console.error('Universal response failed due to unknown error');
      }
      
      throw error;
    }
  }

  // ===== ROBUST JSON PARSING METHOD =====

  /**
   * Parse universal response JSON with robust error handling and cleaning
   */
  private parseUniversalResponseSafely(
    responseText: string, 
    conversationId: string, 
    messageLength: number, 
    hasImages: boolean
  ): UniversalAIResponse | null {
    try {
      // Clean the response text
      let cleanedText = responseText.trim();
      
      // Remove common JSON artifacts
      cleanedText = cleanedText.replace(/^```json\s*/, '').replace(/\s*```$/, '');
      cleanedText = cleanedText.replace(/^```\s*/, '').replace(/\s*```$/, '');
      
      // Remove any leading/trailing whitespace
      cleanedText = cleanedText.trim();
      
      // Check if response starts with JSON object
      if (!cleanedText.startsWith('{')) {
        console.error('Response does not start with JSON object:', {
          conversationId,
          responsePreview: cleanedText.substring(0, 200) + '...',
          originalLength: responseText.length,
          cleanedLength: cleanedText.length
        });
        return null;
      }
      
      // Parse JSON
      const parsed = JSON.parse(cleanedText);
      
      // Validate required fields
      if (!parsed.narrativeResponse) {
        console.warn('Universal response missing narrativeResponse, falling back to basic response', {
          conversationId,
          availableFields: Object.keys(parsed)
        });
        return null;
      }
      
      // Ensure arrays are properly initialized
      parsed.suggestedTasks = parsed.suggestedTasks || [];
      parsed.progressiveInsightUpdates = parsed.progressiveInsightUpdates || [];
      parsed.stateUpdateTags = parsed.stateUpdateTags || [];
      parsed.followUpPrompts = parsed.followUpPrompts || [];
      
      // Add default values for optional fields
      parsed.gamePillData = parsed.gamePillData || null;
      parsed.taskCompletionPrompt = parsed.taskCompletionPrompt || null;
      
      // Add default values for simplified task structure
      if (parsed.suggestedTasks && Array.isArray(parsed.suggestedTasks)) {
        parsed.suggestedTasks = parsed.suggestedTasks.map((task: any) => ({
          title: task.title || '',
          description: task.description || '',
          category: task.category || 'custom',
          confidence: task.confidence || 0.8,
          source: task.source || 'ai_generated'
        }));
      }
      
      return parsed as UniversalAIResponse;
    } catch (error) {
      console.error('JSON Parsing Failed:', {
        errorType: (error as Error).constructor.name,
        errorMessage: (error as Error).message,
        conversationId,
        responseLength: responseText.length,
        responsePreview: responseText.substring(0, 500) + '...',
        messageLength,
        hasImages,
        userTier: 'unknown' // Will be logged separately if needed
      });
      
      // Check for common JSON issues
      if (responseText.includes('```json')) {
        console.error('Response contains markdown code blocks - needs cleaning');
      }
      if (responseText.includes('```')) {
        console.error('Response contains code fences - needs cleaning');
      }
      if (!responseText.trim().startsWith('{')) {
        console.error('Response does not start with JSON object - may have prefix text');
      }
      
      return null;
    }
  }

  // ===== CORE AI METHODS FROM GEMINI SERVICE =====

Â  /**
Â  Â * Send message with streaming support (from geminiService)
Â  Â * This method maintains compatibility with useChat hook
Â  Â */
Â  async sendMessage(
Â  Â  message: string,
Â  Â  conversation: Conversation,
Â  Â  signal: AbortSignal,
Â  Â  onChunk: (chunk: string) => void,
Â  Â  onError: (error: string) => void,
Â  Â  history: ChatMessage[]
Â  ): Promise<void> {
Â  Â  if (await this.checkCooldown()) {
Â  Â  Â  onError('AI service is on cooldown. Please try again later.');
Â  Â  Â  return;
Â  Â  }
Â  Â Â 
Â  Â  // Check universal cache for similar queries
Â  Â  try {
Â  Â  Â  const gameName = conversation.title || undefined;
Â  Â  Â  const genre = conversation.genre || undefined;
Â  Â  Â Â 
Â  Â  Â  const cacheResult = await this.checkAndCacheContent(
Â  Â  Â  Â  message,
Â  Â  Â  Â  'game_help',
Â  Â  Â  Â  gameName,
Â  Â  Â  Â  genre
Â  Â  Â  );
Â  Â  Â Â 
Â  Â  Â  if (cacheResult.found && cacheResult.content) {
Â  Â  Â  Â  console.log(`ğŸ¯ Serving cached game help: ${cacheResult.reason}`);
Â  Â  Â  Â  onChunk(cacheResult.content);
Â  Â  Â  Â  return;
Â  Â  Â  }
Â  Â  } catch (error) {
Â  Â  Â  console.warn('Cache check failed, proceeding with AI generation:', error);
Â  Â  }
Â  Â Â 
Â  Â  try {
Â  Â  Â  const model = this.getOptimalModel('chat');
Â  Â  Â  const chat = await this.getOrCreateChat(conversation, false, model, history);

Â  Â  Â  const streamPromise = chat.sendMessageStream({ message });
Â  Â  Â  const abortPromise = new Promise<never>((_, reject) => {
Â  Â  Â  Â  if (signal.aborted) return reject(new DOMException('Aborted', 'AbortError'));
Â  Â  Â  Â  signal.addEventListener('abort', () => reject(new DOMException('Aborted', 'AbortError')), { once: true });
Â  Â  Â  });

Â  Â  Â  const stream = await Promise.race([streamPromise, abortPromise]);
Â  Â  Â  if (signal.aborted) return;
Â  Â  Â Â 
Â  Â  Â  let fullResponse = '';
Â  Â  Â  await Promise.race([
Â  Â  Â  Â  (async () => {
Â  Â  Â  Â  Â  for await (const chunk of stream) {
Â  Â  Â  Â  Â  Â  if (signal.aborted) break;
Â  Â  Â  Â  Â  Â  if (chunk.text) {
Â  Â  Â  Â  Â  Â  Â  fullResponse += chunk.text;
Â  Â  Â  Â  Â  Â  Â  onChunk(chunk.text);
Â  Â  Â  Â  Â  Â  }
Â  Â  Â  Â  Â  }
Â  Â  Â  Â  })(),
Â  Â  Â  Â  abortPromise
Â  Â  Â  ]);
Â  Â  Â Â 
Â  Â  Â  // Track validation issues (placeholder for future feedback validation)
Â  Â  Â  let validationIssues: string[] = [];

Â  Â  Â  // Track AI response for learning
Â  Â  Â  if (fullResponse) {
Â  Â  Â  Â  await this.trackAIResponse(conversation, message, fullResponse, false, validationIssues);
Â  Â  Â  Â Â 
Â  Â  Â  Â  // Detect progress from user message
Â  Â  Â  Â  try {
Â  Â  Â  Â  Â  const userId = authService.getAuthState().user?.id;
Â  Â  Â  Â  Â  if (userId) {
Â  Â  Â  Â  Â  Â  await this.detectProgressFromResponse(conversation, message, fullResponse, userId);
Â  Â  Â  Â  Â  }
Â  Â  Â  Â  } catch (error) {
Â  Â  Â  Â  Â  console.warn('Progress detection failed:', error);
Â  Â  Â  Â  }
Â  Â  Â  Â Â 
Â  Â  Â  Â  // Cache the generated content for future use
Â  Â  Â  Â  try {
Â  Â  Â  Â  Â  const gameName = conversation.title || undefined;
Â  Â  Â  Â  Â  const genre = conversation.genre || undefined;
Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  await this.cacheGeneratedContent(
Â  Â  Â  Â  Â  Â  message,
Â  Â  Â  Â  Â  Â  fullResponse,
Â  Â  Â  Â  Â  Â  'game_help',
Â  Â  Â  Â  Â  Â  gameName,
Â  Â  Â  Â  Â  Â  genre,
Â  Â  Â  Â  Â  Â  model,
Â  Â  Â  Â  Â  Â  0, // tokens - would need to be calculated
Â  Â  Â  Â  Â  Â  0Â  // cost - would need to be calculated
Â  Â  Â  Â  Â  );
Â  Â  Â  Â  } catch (error) {
Â  Â  Â  Â  Â  console.warn('Failed to cache generated content:', error);
Â  Â  Â  Â  }
Â  Â  Â  }
Â  Â  Â Â 
Â  Â  Â  this.handleSuccess();
Â  Â  } catch (error) {
Â  Â  Â  if (error instanceof DOMException && error.name === 'AbortError') {
Â  Â  Â  Â  console.log("Stream was aborted by user.");
Â  Â  Â  } else {
Â  Â  Â  Â  this.handleError(error, onError);
Â  Â  Â  }
Â  Â  }
Â  }

Â  /**
Â  Â * Send message with images (from geminiService)
Â  Â */
Â  async sendMessageWithImages(
Â  Â  prompt: string,
Â  Â  images: Array<{ base64: string, mimeType: string }>,
Â  Â  conversation: Conversation,
Â  Â  signal: AbortSignal,
Â  Â  onChunk: (chunk: string) => void,
Â  Â  onError: (error: string) => void,
Â  Â  history: ChatMessage[]
Â  ): Promise<void> {
Â  Â  if (await this.checkCooldown()) {
Â  Â  Â  onError('AI service is on cooldown. Please try again later.');
Â  Â  Â  return;
Â  Â  }
Â  Â Â 
Â  Â  try {
Â  Â  Â  const model = this.getOptimalModel('chat_with_images');
Â  Â  Â  const chat = await this.getOrCreateChat(conversation, true, model, history);

Â  Â  Â  const imageParts = images.map(image => ({
Â  Â  Â  Â  inlineData: { data: image.base64, mimeType: image.mimeType }
Â  Â  Â  }));
Â  Â  Â  const textPart = { text: prompt };

Â  Â  Â  const streamPromise = chat.sendMessageStream({
Â  Â  Â  Â  message: [...imageParts, textPart],
Â  Â  Â  });
Â  Â  Â Â 
Â  Â  Â  const abortPromise = new Promise<never>((_, reject) => {
Â  Â  Â  Â  if (signal.aborted) return reject(new DOMException('Aborted', 'AbortError'));
Â  Â  Â  Â  signal.addEventListener('abort', () => reject(new DOMException('Aborted', 'AbortError')), { once: true });
Â  Â  Â  });

Â  Â  Â  const stream = await Promise.race([streamPromise, abortPromise]);
Â  Â  Â  if (signal.aborted) return;
Â  Â  Â Â 
Â  Â  Â  let fullResponse = '';
Â  Â  Â  await Promise.race([
Â  Â  Â  Â  (async () => {
Â  Â  Â  Â  Â  for await (const chunk of stream) {
Â  Â  Â  Â  Â  Â  if (signal.aborted) break;
Â  Â  Â  Â  Â  Â  if (chunk.text) {
Â  Â  Â  Â  Â  Â  Â  fullResponse += chunk.text;
Â  Â  Â  Â  Â  Â  Â  onChunk(chunk.text);
Â  Â  Â  Â  Â  Â  }
Â  Â  Â  Â  Â  }
Â  Â  Â  Â  })(),
Â  Â  Â  Â  abortPromise
Â  Â  Â  ]);
Â  Â  Â Â 
Â  Â  Â  // Track validation issues (placeholder for future feedback validation)
Â  Â  Â  let validationIssues: string[] = [];

Â  Â  Â  // Track AI response for learning
Â  Â  Â  if (fullResponse) {
Â  Â  Â  Â  await this.trackAIResponse(conversation, prompt, fullResponse, true, validationIssues);
Â  Â  Â  Â Â 
Â  Â  Â  Â  // Detect progress from user message
Â  Â  Â  Â  try {
Â  Â  Â  Â  Â  const userId = authService.getAuthState().user?.id;
Â  Â  Â  Â  Â  if (userId) {
Â  Â  Â  Â  Â  Â  await this.detectProgressFromResponse(conversation, prompt, fullResponse, userId);
Â  Â  Â  Â  Â  }
Â  Â  Â  Â  } catch (error) {
Â  Â  Â  Â  Â  console.warn('Progress detection failed:', error);
Â  Â  Â  Â  }
Â  Â  Â  Â Â 
Â  Â  Â  Â  // Cache the generated content for future use
Â  Â  Â  Â  try {
Â  Â  Â  Â  Â  const gameName = conversation.title || undefined;
Â  Â  Â  Â  Â  const genre = conversation.genre || undefined;
Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  await this.cacheGeneratedContent(
Â  Â  Â  Â  Â  Â  prompt,
Â  Â  Â  Â  Â  Â  fullResponse,
Â  Â  Â  Â  Â  Â  'game_help',
Â  Â  Â  Â  Â  Â  gameName,
Â  Â  Â  Â  Â  Â  genre,
Â  Â  Â  Â  Â  Â  model,
Â  Â  Â  Â  Â  Â  0, // tokens - would need to be calculated
Â  Â  Â  Â  Â  Â  0Â  // cost - would need to be calculated
Â  Â  Â  Â  Â  );
Â  Â  Â  Â  } catch (error) {
Â  Â  Â  Â  Â  console.warn('Failed to cache generated content:', error);
Â  Â  Â  Â  }
Â  Â  Â  }
Â  Â  Â Â 
Â  Â  Â  this.handleSuccess();
Â  Â  } catch (error) {
Â  Â  Â  if (error instanceof DOMException && error.name === 'AbortError') {
Â  Â  Â  Â  console.log("Stream was aborted by user.");
Â  Â  Â  } else {
Â  Â  Â  Â  this.handleError(error, onError);
Â  Â  Â  }
Â  Â  }
Â  }

Â  async generateInsight(
Â  Â  gameName: string,
Â  Â  genre: string,
Â  Â  progress: number,
Â  Â  instruction: string,
Â  Â  insightId: string,
Â  Â  onChunk?: (chunk: string) => void,
Â  Â  signal?: AbortSignal
Â  ): Promise<InsightResult> {
Â  Â  try {
Â  Â  Â  // Check cache first
Â  Â  Â  if (this.config.insightCacheEnabled) {
Â  Â  Â  Â  const cached = this.getCachedInsight(gameName, insightId);
Â  Â  Â  Â  if (cached) {
Â  Â  Â  Â  Â  return cached;
Â  Â  Â  Â  }
Â  Â  Â  }

Â  Â  Â  // Get optimal model for insights
Â  Â  Â  const model = this.getOptimalModel('insight_generation');
Â  Â  Â Â 
Â  Â  Â  // Get system instruction
Â  Â  Â  const systemInstruction = this.getInsightSystemInstruction(gameName, genre, progress, instruction, insightId);
Â  Â  Â Â 
Â  Â  Â  // Check user tier for grounding search
Â  Â  Â  let tools: any[] = [];
Â  Â  Â  try {
Â  Â  Â  Â  const userTier = await unifiedUsageService.getTier();
Â  Â  Â  Â  if (userTier === 'pro' || userTier === 'vanguard_pro') {
Â  Â  Â  Â  Â  tools = [{ googleSearch: {} }];
Â  Â  Â  Â  }
Â  Â  Â  } catch (error) {
Â  Â  Â  Â  console.warn('Failed to get user tier for insight generation:', error);
Â  Â  Â  }

Â  Â  Â  // Generate insight
Â  Â  Â  const response = await this.generateContentStream({
Â  Â  Â  Â  model,
Â  Â  Â  Â  contents: `Generate the content for the "${insightId}" insight for the game ${gameName}, following the system instructions.`,
Â  Â  Â  Â  config: { systemInstruction, tools },
Â  Â  Â  Â  onChunk,
Â  Â  Â  Â  signal
Â  Â  Â  });

Â  Â  Â  // Create insight result
Â  Â  Â  const insight: InsightResult = {
Â  Â  Â  Â  tabId: insightId,
Â  Â  Â  Â  title: this.extractTitleFromContent(response),
Â  Â  Â  Â  content: response,
Â  Â  Â  Â  priority: this.determinePriority(insightId, genre),
Â  Â  Â  Â  isProfileSpecific: this.isProfileSpecific(insightId),
Â  Â  Â  Â  generationModel: model.includes('flash') ? 'flash' : 'pro',
Â  Â  Â  Â  lastUpdated: Date.now(),
Â  Â  Â  Â  category: 'enhanced'
Â  Â  Â  };

Â  Â  Â  // Cache the insight
Â  Â  Â  if (this.config.insightCacheEnabled) {
Â  Â  Â  Â  this.cacheInsight(gameName, insight);
Â  Â  Â  }

Â  Â  Â  return insight;
Â  Â  } catch (error) {
Â  Â  Â  console.error('Failed to generate insight:', error);
Â  Â  Â  throw error;
Â  Â  }
Â  }

Â  async generateUnifiedInsights(
Â  Â  gameName: string,
Â  Â  genre: string,
Â  Â  progress: number,
Â  Â  userQuery: string,
Â  Â  signal?: AbortSignal
Â  ): Promise<Record<string, { title: string; content: string }> | null> {
Â  Â  try {
Â  Â  Â  // Check cache first
Â  Â  Â  if (this.config.insightCacheEnabled) {
Â  Â  Â  Â  const cacheKey = `insights_${gameName}_${genre}_${progress}`;
Â  Â  Â  Â  const cached = this.getCachedInsights(cacheKey);
Â  Â  Â  Â  if (cached) {
Â  Â  Â  Â  Â  return cached;
Â  Â  Â  Â  }
Â  Â  Â  }

Â  Â  Â  // Filter tabs that don't require web search
Â  Â  Â  const tabsToGenerate = (insightTabsConfig[genre] || insightTabsConfig.default)
Â  Â  Â  Â  .filter(tab => !tab.webSearch);
Â  Â  Â Â 
Â  Â  Â  if (tabsToGenerate.length === 0) {
Â  Â  Â  Â  return null;
Â  Â  Â  }

Â  Â  Â  // Prepare properties for JSON response
Â  Â  Â  const properties: Record<string, any> = {};
Â  Â  Â  const propertyOrdering: string[] = [];

Â  Â  Â  tabsToGenerate.forEach(tab => {
Â  Â  Â  Â  properties[tab.id] = {
Â  Â  Â  Â  Â  type: 'string',
Â  Â  Â  Â  Â  description: `Content for the ${tab.title} insight tab`
Â  Â  Â  Â  };
Â  Â  Â  Â  propertyOrdering.push(tab.id);
Â  Â  Â  });

Â  Â  Â  // Generate insights
Â  Â  Â  const response = await this.generateContent({
Â  Â  Â  Â  model: this.getOptimalModel('insight_generation'),
Â  Â  Â  Â  contents: `Generate insights for the game ${gameName} (${genre}, ${progress}% progress) based on the user query: "${userQuery}". Generate content for each insight tab.`,
Â  Â  Â  Â  config: {
Â  Â  Â  Â  Â  systemInstruction: this.getUnifiedInsightSystemInstruction(gameName, genre, progress),
Â  Â  Â  Â  Â  responseMimeType: 'application/json',
Â  Â  Â  Â  Â  responseSchema: {
Â  Â  Â  Â  Â  Â  type: 'object',
Â  Â  Â  Â  Â  Â  properties,
Â  Â  Â  Â  Â  Â  required: propertyOrdering
Â  Â  Â  Â  Â  }
Â  Â  Â  Â  },
Â  Â  Â  Â  signal
Â  Â  Â  });

Â  Â  Â  // Parse and cache results
Â  Â  Â  const insights = JSON.parse(response);
Â  Â  Â  if (this.config.insightCacheEnabled) {
Â  Â  Â  Â  this.cacheInsights(`insights_${gameName}_${genre}_${progress}`, insights);
Â  Â  Â  }

Â  Â  Â  return insights;
Â  Â  } catch (error) {
Â  Â  Â  console.error('Failed to generate unified insights:', error);
Â  Â  Â  return null;
Â  Â  }
Â  }

Â  // ===== PROACTIVE INSIGHTS =====

Â  async processProactiveTrigger(trigger: ProactiveTrigger): Promise<ProactiveInsightSuggestion[]> {
Â  Â  if (!this.config.useProactiveInsights) {
Â  Â  Â  return [];
Â  Â  }

Â  Â  try {
Â  Â  Â  // Get player profile
Â  Â  Â  const profile = await this.getPlayerProfile(trigger.gameId);
Â  Â  Â  if (!profile) {
Â  Â  Â  Â  return [];
Â  Â  Â  }

Â  Â  Â  // Get game context
Â  Â  Â  const gameContext = await this.getGameContext(trigger.gameId);
Â  Â  Â  if (!gameContext) {
Â  Â  Â  Â  return [];
Â  Â  Â  }

Â  Â  Â  // Generate insights based on trigger type
Â  Â  Â  const insights = await this.generateInsightsForTrigger(trigger, profile, gameContext);
Â  Â  Â Â 
Â  Â  Â  return insights;
Â  Â  } catch (error) {
Â  Â  Â  console.error('Failed to process proactive trigger:', error);
Â  Â  Â  return [];
Â  Â  }
Â  }

Â  private async generateInsightsForTrigger(
Â  Â  trigger: ProactiveTrigger,
Â  Â  profile: PlayerProfile,
Â  Â  gameContext: GameContext
Â  ): Promise<ProactiveInsightSuggestion[]> {
Â  Â  const insights: ProactiveInsightSuggestion[] = [];
Â  Â Â 
Â  Â  switch (trigger.type) {
Â  Â  Â  case 'objective_complete':
Â  Â  Â  Â  insights.push(...this.generateObjectiveCompleteInsights(trigger, profile, gameContext));
Â  Â  Â  Â  break;
Â  Â  Â  case 'inventory_change':
Â  Â  Â  Â  insights.push(...this.generateInventoryChangeInsights(trigger, profile, gameContext));
Â  Â  Â  Â  break;
Â  Â  Â  case 'area_discovery':
Â  Â  Â  Â  insights.push(...this.generateAreaDiscoveryInsights(trigger, profile, gameContext));
Â  Â  Â  Â  break;
Â  Â  Â  case 'session_start':
Â  Â  Â  Â  insights.push(...this.generateSessionStartInsights(trigger, profile, gameContext));
Â  Â  Â  Â  break;
Â  Â  Â  case 'session_end':
Â  Â  Â  Â  insights.push(...this.generateSessionEndInsights(trigger, profile, gameContext));
Â  Â  Â  Â  break;
Â  Â  Â  case 'progress_milestone':
Â  Â  Â  Â  insights.push(...this.generateProgressMilestoneInsights(trigger, profile, gameContext));
Â  Â  Â  Â  break;
Â  Â  Â  case 'difficulty_spike':
Â  Â  Â  Â  insights.push(...this.generateDifficultySpikeInsights(trigger, profile, gameContext));
Â  Â  Â  Â  break;
Â  Â  Â  case 'exploration_pattern':
Â  Â  Â  Â  insights.push(...this.generateExplorationPatternInsights(trigger, profile, gameContext));
Â  Â  Â  Â  break;
Â  Â  }
Â  Â Â 
Â  Â  return insights;
Â  }

Â  // ===== PROFILE-AWARE INSIGHTS =====

Â  async generateProfileAwareInsights(
Â  Â  gameName: string,
Â  Â  genre: string,
Â  Â  progress: number,
Â  Â  userQuery: string,
Â  Â  signal?: AbortSignal
Â  ): Promise<InsightResult[]> {
Â  Â  if (!this.config.useProfileAwareInsights) {
Â  Â  Â  return [];
Â  Â  }

Â  Â  try {
Â  Â  Â  // Get player profile
Â  Â  Â  const profile = await this.getPlayerProfile(gameName);
Â  Â  Â  if (!profile) {
Â  Â  Â  Â  return [];
Â  Â  Â  }

Â  Â  Â  // Get game context
Â  Â  Â  const gameContext = await this.getGameContext(gameName);
Â  Â  Â  if (!gameContext) {
Â  Â  Â  Â  return [];
Â  Â  Â  }

Â  Â  Â  // Generate profile-aware tabs
Â  Â  Â  const tabs = this.generateProfileAwareTabs(genre, profile, gameContext);
Â  Â  Â Â 
Â  Â  Â  // Generate insights for each tab
Â  Â  Â  const insights: InsightResult[] = [];
Â  Â  Â  for (const tab of tabs) {
Â  Â  Â  Â  try {
Â  Â  Â  Â  Â  const insight = await this.generateInsight(
Â  Â  Â  Â  Â  Â  gameName,
Â  Â  Â  Â  Â  Â  genre,
Â  Â  Â  Â  Â  Â  progress,
Â  Â  Â  Â  Â  Â  tab.instruction || '',
Â  Â  Â  Â  Â  Â  tab.id,
Â  Â  Â  Â  Â  Â  undefined,
Â  Â  Â  Â  Â  Â  signal
Â  Â  Â  Â  Â  );
Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  insights.push({
Â  Â  Â  Â  Â  Â  ...insight,
Â  Â  Â  Â  Â  Â  category: 'profile_aware',
Â  Â  Â  Â  Â  Â  isProfileSpecific: true
Â  Â  Â  Â  Â  });
Â  Â  Â  Â  } catch (error) {
Â  Â  Â  Â  Â  console.warn(`Failed to generate insight for tab ${tab.id}:`, error);
Â  Â  Â  Â  }
Â  Â  Â  }

Â  Â  Â  return insights;
Â  Â  } catch (error) {
Â  Â  Â  console.error('Failed to generate profile-aware insights:', error);
Â  Â  Â  return [];
Â  Â  }
Â  }

Â  // ===== SUGGESTED PROMPTS =====

Â  async generateSuggestedPrompts(
Â  Â  context: string,
Â  Â  gameName?: string,
Â  Â  maxPrompts: number = 4
Â  ): Promise<PromptSuggestion[]> {
Â  Â  try {
Â  Â  Â  const prompts: PromptSuggestion[] = [];
Â  Â  Â Â 
Â  Â  Â  // Generate contextual prompts
Â  Â  Â  const contextualPrompts = this.generateContextualPrompts(context, gameName);
Â  Â  Â  prompts.push(...contextualPrompts);
Â  Â  Â Â 
Â  Â  Â  // Generate follow-up prompts
Â  Â  Â  const followUpPrompts = this.generateFollowUpPrompts(context);
Â  Â  Â  prompts.push(...followUpPrompts);
Â  Â  Â Â 
Â  Â  Â  // Filter out used prompts
Â  Â  Â  const availablePrompts = prompts.filter(p => !this.usedPrompts.has(p.text));
Â  Â  Â Â 
Â  Â  Â  // Sort by priority and return top prompts
Â  Â  Â  return availablePrompts
Â  Â  Â  Â  .sort((a, b) => b.priority - a.priority)
Â  Â  Â  Â  .slice(0, maxPrompts);
Â  Â  } catch (error) {
Â  Â  Â  console.error('Failed to generate suggested prompts:', error);
Â  Â  Â  return [];
Â  Â  }
Â  }

Â  markPromptAsUsed(prompt: string): void {
Â  Â  this.usedPrompts.add(prompt);
Â  Â  this.saveUsedPrompts();
Â  }

Â  isPromptUsed(prompt: string): boolean {
Â  Â  return this.usedPrompts.has(prompt);
Â  }

Â  // ===== UTILITY METHODS =====

Â  private async checkCooldown(): Promise<boolean> {
Â  Â  try {
Â  Â  Â  const cooldownEnd = localStorage.getItem('geminiCooldownEnd');
Â  Â  Â  if (cooldownEnd) {
Â  Â  Â  Â  const endTime = parseInt(cooldownEnd);
Â  Â  Â  Â  if (Date.now() < endTime) {
Â  Â  Â  Â  Â  return true;
Â  Â  Â  Â  }
Â  Â  Â  }
Â  Â  Â  return false;
Â  Â  } catch (error) {
Â  Â  Â  return false;
Â  Â  }
Â  }

  private getOptimalModel(task: 'chat' | 'chat_with_images' | 'insight_generation' | 'image_analysis'): GeminiModel {
Â  Â  if (!this.config.costOptimization) {
Â  Â  Â  return 'gemini-2.5-pro';
Â  Â  }

Â  Â  // Cost optimization strategy
Â  Â  switch (task) {
Â  Â  Â  case 'chat':
Â  Â  Â  Â  return 'gemini-2.5-flash';
Â  Â  Â  case 'insight_generation':
Â  Â  Â  Â  return 'gemini-2.5-flash';
Â  Â  Â  case 'image_analysis':
Â  Â  Â  Â  return 'gemini-2.5-flash';
Â  Â  Â  default:
Â  Â  Â  Â  return 'gemini-2.5-flash';
Â  Â  }
Â  }

Â  private async getSystemInstruction(conversation: Conversation, hasImages: boolean): Promise<string> {
Â  Â  const userFirstName = await profileService.getName();
Â  Â  const baseDirectives = `You are Otakon, an AI gaming assistant. Address the user as ${userFirstName || 'friend'}.`;
Â  Â Â 
Â  Â  if (hasImages) {
Â  Â  Â  return this.getImageAnalysisSystemInstruction(baseDirectives);
Â  Â  } else {
Â  Â  Â  return this.getChatSystemInstruction(baseDirectives);
Â  Â  }
Â  }

Â  private getImageAnalysisSystemInstruction(baseDirectives: string): string {
Â  Â  return `${baseDirectives}

**OTAKON MASTER PROMPT V19 - SCREENSHOT ANALYSIS**

Core Protocols & Tags (Execute ONE most relevant protocol per response):

* **Game Identification & Analysis (CRITICAL FIRST STEP):**
Â  * **Initial Visual Identification:** Analyze the image to form a hypothesis about the game's identity
Â  * **CRITICAL VERIFICATION VIA SEARCH:** Use search tool to confirm your hypothesis
Â  * **Verify Release Date:** Find the official release date to determine status
Â  * **Response Tags:** Your response MUST begin with:
Â  Â  * \`[OTAKON_GAME_ID: The Full Name of the Game]\`
Â  Â  * \`[OTAKON_CONFIDENCE: high|low]\`
Â  * **Then, based on verified release status:**
Â  Â  * **If Released (and confidence is high):**
Â  Â  Â  * Include \`[OTAKON_GAME_PROGRESS: <number>]\`
Â  Â  Â  * Include \`[OTAKON_GENRE: <Primary Game Genre>]\`
Â  Â  * **If Unreleased:**
Â  Â  Â  * Include \`[OTAKON_GAME_STATUS: unreleased]\`

* **Analysis & Assistance:**
Â  * Provide detailed analysis of what's shown in the screenshot
Â  * Offer helpful suggestions and tips
Â  * Answer any specific questions about the game

* **Formatting Rules:**
Â  * Use clear, engaging language
Â  * Include relevant game information
Â  * Provide actionable advice

* **Suggestions:**
Â  * End with \`[OTAKON_SUGGESTIONS: ["suggestion 1", "suggestion 2", "suggestion 3", "suggestion 4"]]\`
Â  * Make suggestions inquisitive questions to guide the user`;
Â  }

Â  private getChatSystemInstruction(baseDirectives: string): string {
Â  Â  return `${baseDirectives}

**OTAKON MASTER PROMPT V19 - CHAT ASSISTANCE**

Core Protocols:

* **Game Assistance:**
Â  * Provide helpful information about games
Â  * Offer tips, strategies, and insights
Â  * Answer questions about gameplay, mechanics, and lore

* **Formatting Rules:**
Â  * Use clear, engaging language
Â  * Include relevant information
Â  * Provide actionable advice

* **Suggestions:**
Â  * End with \`[OTAKON_SUGGESTIONS: ["suggestion 1", "suggestion 2", "suggestion 3", "suggestion 4"]]\`
Â  * Make suggestions inquisitive questions to guide the user`;
Â  }

Â  private getInsightSystemInstruction(
Â  Â  gameName: string,
Â  Â  genre: string,
Â  Â  progress: number,
Â  Â  instruction: string,
Â  Â  insightId: string
Â  ): string {
Â  Â  return `You are generating insights for the game "${gameName}" (${genre}, ${progress}% progress).

**CRITICAL CONTENT RULES (Non-negotiable):**
1. **DETAIL AND DEPTH:** Generate detailed, wiki-style content that is comprehensive and thorough. Avoid short, superficial descriptions. Provide rich, useful information that adds significant value to the player's experience.
2. **WIKI-STYLE FORMATTING:** Structure content like a game wiki with clear sections, detailed explanations, and comprehensive coverage of all relevant aspects up to the current progress point.
3. **STRICT SPOILER-GATING:** All information provided MUST be relevant and accessible to a player who is ${progress}% through the game. You are strictly forbidden from mentioning, hinting at, or alluding to any characters, locations, items, or plot points that appear after this progress marker.
4. **COMPREHENSIVE COVERAGE:** Provide comprehensive information that covers ALL relevant aspects up to the current progress point. Don't hold back on details - the player has experienced everything up to ${progress}% and deserves full context and analysis.
5. **ACTIONABLE CONTENT:** Include specific, actionable advice, strategies, and information that the player can immediately use to enhance their gameplay experience.

**FORMATTING REQUIREMENTS:**
- Use clear Markdown headings (##, ###) to structure content
- Include bullet points and lists for better readability
- Content should be substantial (300-800 words) with detailed, actionable information
- Write in an informative, wiki-style tone that's both comprehensive and accessible

Insight ID: ${insightId}
Instruction: ${instruction}

Generate comprehensive, detailed wiki-style content that provides maximum value to the player at their current progress level.`;
Â  }

Â  private getUnifiedInsightSystemInstruction(
Â  Â  gameName: string,
Â  Â  genre: string,
Â  Â  progress: number
Â  ): string {
Â  Â  return `You are generating multiple insights for the game "${gameName}" (${genre}, ${progress}% progress).

**CRITICAL CONTENT RULES (Non-negotiable):**
1. **DETAIL AND DEPTH:** Each insight tab must contain detailed, wiki-style content that is comprehensive and thorough. Avoid short, superficial descriptions. Provide rich, useful information that adds significant value to the player's experience.
2. **WIKI-STYLE FORMATTING:** Structure content like a game wiki with clear sections, detailed explanations, and comprehensive coverage of all relevant aspects up to the current progress point.
3. **STRICT SPOILER-GATING:** All information provided MUST be relevant and accessible to a player who is ${progress}% through the game. You are strictly forbidden from mentioning, hinting at, or alluding to any characters, locations, items, or plot points that appear after this progress marker.
4. **COMPREHENSIVE COVERAGE:** For each tab, provide comprehensive information that covers ALL relevant aspects up to the current progress point. Don't hold back on details - the player has experienced everything up to ${progress}% and deserves full context and analysis.
5. **ACTIONABLE CONTENT:** Include specific, actionable advice, strategies, and information that the player can immediately use to enhance their gameplay experience.

**FORMATTING REQUIREMENTS:**
- Use clear Markdown headings (##, ###) to structure content
- Include bullet points and lists for better readability
- Each tab should be substantial (300-800 words) with detailed, actionable information
- Write in an informative, wiki-style tone that's both comprehensive and accessible

Generate comprehensive, detailed wiki-style content for each insight tab that provides maximum value to the player at their current progress level.`;
Â  }

Â  private prepareContent(message: string, hasImages: boolean): any {
Â  Â  if (hasImages) {
Â  Â  Â  // Handle image content
Â  Â  Â  return message; // This would need to be processed for images
Â  Â  } else {
Â  Â  Â  return message;
Â  Â  }
Â  }

Â  private async generateContent(params: {
Â  Â  model: GeminiModel;
Â  Â  contents: any;
Â  Â  config: any;
Â  Â  signal?: AbortSignal;
Â  }): Promise<string> {
Â  Â  const { model, contents, config, signal } = params;
Â  Â Â 
Â  Â  try {
Â  Â  Â  // Ensure AI is initialized before use
Â  Â  Â  this.ensureAIInitialized();
Â  Â  Â Â 
Â  Â  Â  if (!this.ai) {
Â  Â  Â  Â  throw new Error('AI service not available: No API key provided');
Â  Â  Â  }

Â  Â  Â  const response = await this.ai.models.generateContent({
Â  Â  Â  Â  model,
Â  Â  Â  Â  contents,
Â  Â  Â  Â  config
Â  Â  Â  });
Â  Â  Â Â 
Â  Â  Â  return response.text || '';
Â  Â  } catch (error) {
Â  Â  Â  if (this.isQuotaError(error)) {
Â  Â  Â  Â  await this.setCooldown();
Â  Â  Â  Â  throw new Error('API quota exceeded. Please try again later.');
Â  Â  Â  }
Â  Â  Â  throw error;
Â  Â  }
Â  }

Â  private async generateContentStream(params: {
Â  Â  model: GeminiModel;
Â  Â  contents: any;
Â  Â  config: any;
Â  Â  onChunk?: (chunk: string) => void;
Â  Â  signal?: AbortSignal;
Â  }): Promise<string> {
Â  Â  const { model, contents, config, onChunk, signal } = params;
Â  Â Â 
Â  Â  try {
Â  Â  Â  // Ensure AI is initialized before use
Â  Â  Â  this.ensureAIInitialized();
Â  Â  Â Â 
Â  Â  Â  if (!this.ai) {
Â  Â  Â  Â  throw new Error('AI service not available: No API key provided');
Â  Â  Â  }

Â  Â  Â  const stream = await this.ai.models.generateContentStream({
Â  Â  Â  Â  model,
Â  Â  Â  Â  contents,
Â  Â  Â  Â  config
Â  Â  Â  });
Â  Â  Â Â 
Â  Â  Â  let fullResponse = '';
Â  Â  Â  for await (const chunk of stream) {
Â  Â  Â  Â  if (signal?.aborted) break;
Â  Â  Â  Â Â 
Â  Â  Â  Â  const text = chunk.text || '';
Â  Â  Â  Â  fullResponse += text;
Â  Â  Â  Â  onChunk?.(text);
Â  Â  Â  }
Â  Â  Â Â 
Â  Â  Â  return fullResponse;
Â  Â  } catch (error) {
Â  Â  Â  if (this.isQuotaError(error)) {
Â  Â  Â  Â  await this.setCooldown();
Â  Â  Â  Â  throw new Error('API quota exceeded. Please try again later.');
Â  Â  Â  }
Â  Â  Â  throw error;
Â  Â  }
Â  }

Â  private async processResponse(response: string, model: GeminiModel): Promise<AIResponse> {
Â  Â  // Extract suggestions
Â  Â  const suggestions = this.extractSuggestions(response);
Â  Â Â 
Â  Â  // Extract game info
Â  Â  const gameInfo = this.extractGameInfo(response);
Â  Â Â 
Â  Â  // Clean response content
Â  Â  const content = this.cleanResponseContent(response);
Â  Â Â 
Â  Â  return {
Â  Â  Â  content,
Â  Â  Â  suggestions,
Â  Â  Â  gameInfo,
Â  Â  Â  metadata: {
Â  Â  Â  Â  model,
Â  Â  Â  Â  timestamp: Date.now(),
Â  Â  Â  Â  cost: this.calculateCost(model, content.length),
Â  Â  Â  Â  tokens: this.estimateTokens(content)
Â  Â  Â  }
Â  Â  };
Â  }

Â  private extractSuggestions(response: string): string[] {
    const suggestionsMatch = response.match(/\[OTAKON_SUGGESTIONS:\s*\[(.*?)\]\]/);
Â  Â  if (suggestionsMatch) {
Â  Â  Â  try {
Â  Â  Â  Â  const suggestionsArray = JSON.parse(`[${suggestionsMatch[1]}]`);
Â  Â  Â  Â  return Array.isArray(suggestionsArray) ? suggestionsArray : [];
Â  Â  Â  } catch (error) {
Â  Â  Â  Â  console.warn('Failed to parse suggestions:', error);
Â  Â  Â  }
Â  Â  }
Â  Â  return [];
Â  }

Â  private extractGameInfo(response: string): AIResponse['gameInfo'] {
Â  Â  const gameIdMatch = response.match(/\[OTAKON_GAME_ID:\s*(.*?)\]/);
Â  Â  const confidenceMatch = response.match(/\[OTAKON_CONFIDENCE:\s*(.*?)\]/);
Â  Â  const progressMatch = response.match(/\[OTAKON_GAME_PROGRESS:\s*(.*?)\]/);
Â  Â  const genreMatch = response.match(/\[OTAKON_GENRE:\s*(.*?)\]/);
Â  Â Â 
Â  Â  if (gameIdMatch && confidenceMatch) {
Â  Â  Â  return {
Â  Â  Â  Â  gameId: gameIdMatch[1].trim(),
Â  Â  Â  Â  confidence: confidenceMatch[1].trim() as 'high' | 'low',
Â  Â  Â  Â  progress: progressMatch ? parseInt(progressMatch[1]) : undefined,
Â  Â  Â  Â  genre: genreMatch ? genreMatch[1].trim() : undefined
Â  Â  Â  };
Â  Â  }
Â  Â Â 
Â  Â  return undefined;
Â  }

Â  private cleanResponseContent(response: string): string {
Â  Â  // Remove OTAKON tags
Â  Â  return response
Â  Â  Â  .replace(/\[OTAKON_[^\]]*\]/g, '')
      .replace(/\[OTAKON_SUGGESTIONS:.*?\]/g, '')
Â  Â  Â  .trim();
Â  }

Â  private calculateCost(model: GeminiModel, contentLength: number): number {
Â  Â  // Simplified cost calculation
Â  Â  const tokens = this.estimateTokens(contentLength.toString());
Â  Â  const costPerToken = model.includes('pro') ? 0.00001 : 0.000001;
Â  Â  return tokens * costPerToken;
Â  }

Â  private estimateTokens(text: string | number): number {
Â  Â  const textStr = text.toString();
Â  Â  return Math.ceil(textStr.length / 4); // Rough estimation
Â  }

Â  private isQuotaError(error: any): boolean {
Â  Â  const errorMessage = error.toString();
Â  Â  const httpStatus = error.httpError?.status;
Â  Â  return errorMessage.includes("RESOURCE_EXHAUSTED") || httpStatus === 429;
Â  }

Â  private async setCooldown(): Promise<void> {
Â  Â  const cooldownEnd = Date.now() + this.COOLDOWN_DURATION;
Â  Â  localStorage.setItem('geminiCooldownEnd', cooldownEnd.toString());
Â  }

Â  private async updateUsageTracking(metadata: AIResponse['metadata']): Promise<void> {
Â  Â  try {
Â  Â  Â  await apiCostService.recordAPICall(
Â  Â  Â  Â  metadata.model === 'gemini-2.5-pro' ? 'pro' : 'flash',
Â  Â  Â  Â  'user_query',
Â  Â  Â  Â  'paid', // This should be determined by user tier
Â  Â  Â  Â  metadata.tokens,
Â  Â  Â  Â  true,
Â  Â  Â  Â  undefined,
Â  Â  Â  Â  undefined,
Â  Â  Â  Â  undefined,
Â  Â  Â  Â  undefined,
Â  Â  Â  Â  undefined,
Â  Â  Â  Â  { cost: metadata.cost }
Â  Â  Â  );
Â  Â  } catch (error) {
Â  Â  Â  console.warn('Failed to update usage tracking:', error);
Â  Â  }
Â  }

Â  // ===== INSIGHT GENERATION HELPERS =====

Â  private extractTitleFromContent(content: string): string {
Â  Â  // Extract title from content (first line or first sentence)
Â  Â  const lines = content.split('\n');
Â  Â  const firstLine = lines[0]?.trim();
Â  Â  if (firstLine && firstLine.length < 100) {
Â  Â  Â  return firstLine;
Â  Â  }
Â  Â Â 
Â  Â  const sentences = content.split('.');
Â  Â  const firstSentence = sentences[0]?.trim();
Â  Â  if (firstSentence && firstSentence.length < 100) {
Â  Â  Â  return firstSentence;
Â  Â  }
Â  Â Â 
Â  Â  return 'Insight';
Â  }

Â  private determinePriority(insightId: string, genre: string): 'high' | 'medium' | 'low' {
Â  Â  // Determine priority based on insight ID and genre
Â  Â  const highPriorityInsights = ['strategy', 'tips', 'walkthrough'];
Â  Â  const mediumPriorityInsights = ['lore', 'characters', 'items'];
Â  Â Â 
Â  Â  if (highPriorityInsights.some(id => insightId.includes(id))) {
Â  Â  Â  return 'high';
Â  Â  } else if (mediumPriorityInsights.some(id => insightId.includes(id))) {
Â  Â  Â  return 'medium';
Â  Â  }
Â  Â Â 
Â  Â  return 'low';
Â  }

Â  private isProfileSpecific(insightId: string): boolean {
Â  Â  // Determine if insight is profile-specific
Â  Â  const profileSpecificInsights = ['personalized', 'recommended', 'suggested'];
Â  Â  return profileSpecificInsights.some(id => insightId.includes(id));
Â  }

Â  // ===== CACHING METHODS =====

Â  private getCachedInsight(gameName: string, insightId: string): InsightResult | null {
Â  Â  const cacheKey = `${gameName}_${insightId}`;
Â  Â  const cached = this.insightCache.get(cacheKey);
Â  Â  if (cached && cached.length > 0) {
Â  Â  Â  return cached[0];
Â  Â  }
Â  Â  return null;
Â  }

Â  private getCachedInsights(cacheKey: string): Record<string, { title: string; content: string }> | null {
Â  Â  // This would need to be implemented based on the cache structure
Â  Â  return null;
Â  }

Â  private cacheInsight(gameName: string, insight: InsightResult): void {
Â  Â  const cacheKey = `${gameName}_${insight.tabId}`;
Â  Â  this.insightCache.set(cacheKey, [insight]);
Â  Â  this.saveInsightCache();
Â  }

Â  private cacheInsights(cacheKey: string, insights: Record<string, { title: string; content: string }>): void {
Â  Â  // This would need to be implemented based on the cache structure
Â  }

Â  private loadInsightCache(): void {
Â  Â  try {
Â  Â  Â  const cached = localStorage.getItem('otakon_insight_cache');
Â  Â  Â  if (cached) {
Â  Â  Â  Â  this.insightCache = new Map(JSON.parse(cached));
Â  Â  Â  }
Â  Â  } catch (error) {
Â  Â  Â  console.warn('Failed to load insight cache:', error);
Â  Â  }
Â  }

Â  private saveInsightCache(): void {
Â  Â  try {
Â  Â  Â  const cacheArray = Array.from(this.insightCache.entries());
Â  Â  Â  localStorage.setItem('otakon_insight_cache', JSON.stringify(cacheArray));
Â  Â  } catch (error) {
Â  Â  Â  console.warn('Failed to save insight cache:', error);
Â  Â  }
Â  }

Â  // ===== PROMPT SUGGESTION HELPERS =====

Â  private generateContextualPrompts(context: string, gameName?: string): PromptSuggestion[] {
Â  Â  const prompts: PromptSuggestion[] = [];
Â  Â Â 
Â  Â  if (gameName) {
Â  Â  Â  prompts.push({
Â  Â  Â  Â  id: `game_${Date.now()}_1`,
Â  Â  Â  Â  text: `Tell me more about ${gameName}`,
Â  Â  Â  Â  category: 'game_specific',
Â  Â  Â  Â  priority: 8,
Â  Â  Â  Â  used: false,
Â  Â  Â  Â  metadata: { gameName }
Â  Â  Â  });
Â  Â  Â Â 
Â  Â  Â  prompts.push({
Â  Â  Â  Â  id: `game_${Date.now()}_2`,
Â  Â  Â  Â  text: `What are some tips for ${gameName}?`,
Â  Â  Â  Â  category: 'game_specific',
Â  Â  Â  Â  priority: 7,
Â  Â  Â  Â  used: false,
Â  Â  Â  Â  metadata: { gameName }
Â  Â  Â  });
Â  Â  }
Â  Â Â 
Â  Â  prompts.push({
Â  Â  Â  id: `context_${Date.now()}_1`,
Â  Â  Â  text: 'What should I do next?',
Â  Â  Â  category: 'contextual',
Â  Â  Â  priority: 6,
Â  Â  Â  used: false,
Â  Â  Â  metadata: { context }
Â  Â  });
Â  Â Â 
Â  Â  return prompts;
Â  }

Â  private generateFollowUpPrompts(context: string): PromptSuggestion[] {
Â  Â  return [
Â  Â  Â  {
Â  Â  Â  Â  id: `followup_${Date.now()}_1`,
Â  Â  Â  Â  text: 'Can you explain that in more detail?',
Â  Â  Â  Â  category: 'follow_up',
Â  Â  Â  Â  priority: 5,
Â  Â  Â  Â  used: false,
Â  Â  Â  Â  metadata: { context }
Â  Â  Â  },
Â  Â  Â  {
Â  Â  Â  Â  id: `followup_${Date.now()}_2`,
Â  Â  Â  Â  text: 'What are some alternatives?',
Â  Â  Â  Â  category: 'follow_up',
Â  Â  Â  Â  priority: 4,
Â  Â  Â  Â  used: false,
Â  Â  Â  Â  metadata: { context }
Â  Â  Â  }
Â  Â  ];
Â  }

Â  private loadUsedPrompts(): void {
Â  Â  try {
Â  Â  Â  const used = localStorage.getItem('otakon_used_prompts');
Â  Â  Â  if (used) {
Â  Â  Â  Â  this.usedPrompts = new Set(JSON.parse(used));
Â  Â  Â  }
Â  Â  } catch (error) {
Â  Â  Â  console.warn('Failed to load used prompts:', error);
Â  Â  }
Â  }

Â  private saveUsedPrompts(): void {
Â  Â  try {
Â  Â  Â  localStorage.setItem('otakon_used_prompts', JSON.stringify(Array.from(this.usedPrompts)));
Â  Â  } catch (error) {
Â  Â  Â  console.warn('Failed to save used prompts:', error);
Â  Â  }
Â  }

Â  // ===== PROACTIVE INSIGHT HELPERS =====

Â  private generateObjectiveCompleteInsights(
Â  Â  trigger: ProactiveTrigger,
Â  Â  profile: PlayerProfile,
Â  Â  gameContext: GameContext
Â  ): ProactiveInsightSuggestion[] {
Â  Â  return [{
Â  Â  Â  id: `objective_${Date.now()}`,
Â  Â  Â  title: 'Objective Complete!',
Â  Â  Â  content: `Great job completing that objective! Here are some suggestions for what to do next.`,
Â  Â  Â  priority: 'high',
Â  Â  Â  triggerType: 'objective_complete',
Â  Â  Â  gameId: trigger.gameId,
Â  Â  Â  timestamp: Date.now(),
Â  Â  Â  metadata: { objective: trigger.data }
Â  Â  }];
Â  }

Â  private generateInventoryChangeInsights(
Â  Â  trigger: ProactiveTrigger,
Â  Â  profile: PlayerProfile,
Â  Â  gameContext: GameContext
Â  ): ProactiveInsightSuggestion[] {
Â  Â  return [{
Â  Â  Â  id: `inventory_${Date.now()}`,
Â  Â  Â  title: 'Inventory Updated',
Â  Â  Â  content: `Your inventory has changed. Here are some tips for managing your items.`,
Â  Â  Â  priority: 'medium',
Â  Â  Â  triggerType: 'inventory_change',
Â  Â  Â  gameId: trigger.gameId,
Â  Â  Â  timestamp: Date.now(),
Â  Â  Â  metadata: { inventory: trigger.data }
Â  Â  }];
Â  }

Â  private generateAreaDiscoveryInsights(
Â  Â  trigger: ProactiveTrigger,
Â  Â  profile: PlayerProfile,
Â  Â  gameContext: GameContext
Â  ): ProactiveInsightSuggestion[] {
Â  Â  return [{
Â  Â  Â  id: `area_${Date.now()}`,
Â  Â  Â  title: 'New Area Discovered!',
Â  Â  Â  content: `You've discovered a new area! Here's what you should know about this location.`,
Â  Â  Â  priority: 'high',
Â  Â  Â  triggerType: 'area_discovery',
Â  Â  Â  gameId: trigger.gameId,
Â  Â  Â  timestamp: Date.now(),
Â  Â  Â  metadata: { area: trigger.data }
Â  Â  }];
Â  }

Â  private generateSessionStartInsights(
Â  Â  trigger: ProactiveTrigger,
Â  Â  profile: PlayerProfile,
Â  Â  gameContext: GameContext
Â  ): ProactiveInsightSuggestion[] {
Â  Â  return [{
Â  Â  Â  id: `session_start_${Date.now()}`,
Â  Â  Â  title: 'Welcome Back!',
Â  Â  Â  content: `Welcome back to ${trigger.gameTitle}! Here's what you were working on.`,
Â  Â  Â  priority: 'medium',
Â  Â  Â  triggerType: 'session_start',
Â  Â  Â  gameId: trigger.gameId,
Â  Â  Â  timestamp: Date.now(),
Â  Â  Â  metadata: { session: trigger.data }
Â  Â  }];
Â  }

Â  private generateSessionEndInsights(
Â  Â  trigger: ProactiveTrigger,
Â  Â  profile: PlayerProfile,
Â  Â  gameContext: GameContext
Â  ): ProactiveInsightSuggestion[] {
Â  Â  return [{
Â  Â  Â  id: `session_end_${Date.now()}`,
Â  Â  Â  title: 'Session Summary',
Â  Â  Â  content: `Great session! Here's a summary of what you accomplished.`,
Â  Â  Â  priority: 'low',
Â  Â  Â  triggerType: 'session_end',
Â  Â  Â  gameId: trigger.gameId,
Â  Â  Â  timestamp: Date.now(),
Â  Â  Â  metadata: { session: trigger.data }
Â  Â  }];
Â  }

Â  private generateProgressMilestoneInsights(
Â  Â  trigger: ProactiveTrigger,
Â  Â  profile: PlayerProfile,
Â  Â  gameContext: GameContext
Â  ): ProactiveInsightSuggestion[] {
Â  Â  return [{
Â  Â  Â  id: `milestone_${Date.now()}`,
Â  Â  Â  title: 'Progress Milestone!',
Â  Â  Â  content: `Congratulations on reaching a progress milestone! Here's what's next.`,
Â  Â  Â  priority: 'high',
Â  Â  Â  triggerType: 'progress_milestone',
Â  Â  Â  gameId: trigger.gameId,
Â  Â  Â  timestamp: Date.now(),
Â  Â  Â  metadata: { milestone: trigger.data }
Â  Â  }];
Â  }

Â  private generateDifficultySpikeInsights(
Â  Â  trigger: ProactiveTrigger,
Â  Â  profile: PlayerProfile,
Â  Â  gameContext: GameContext
Â  ): ProactiveInsightSuggestion[] {
Â  Â  return [{
Â  Â  Â  id: `difficulty_${Date.now()}`,
Â  Â  Â  title: 'Difficulty Spike Detected',
Â  Â  Â  content: `The game seems to be getting harder. Here are some strategies to help you through this challenge.`,
Â  Â  Â  priority: 'high',
Â  Â  Â  triggerType: 'difficulty_spike',
Â  Â  Â  gameId: trigger.gameId,
Â  Â  Â  timestamp: Date.now(),
Â  Â  Â  metadata: { difficulty: trigger.data }
Â  Â  }];
Â  }

Â  private generateExplorationPatternInsights(
Â  Â  trigger: ProactiveTrigger,
Â  Â  profile: PlayerProfile,
Â  Â  gameContext: GameContext
Â  ): ProactiveInsightSuggestion[] {
Â  Â  return [{
Â  Â  Â  id: `exploration_${Date.now()}`,
Â  Â  Â  title: 'Exploration Pattern',
Â  Â  Â  content: `I've noticed your exploration pattern. Here are some suggestions for areas you might want to check out.`,
Â  Â  Â  priority: 'medium',
Â  Â  Â  triggerType: 'exploration_pattern',
Â  Â  Â  gameId: trigger.gameId,
Â  Â  Â  timestamp: Date.now(),
Â  Â  Â  metadata: { pattern: trigger.data }
Â  Â  }];
Â  }

Â  // ===== PROFILE-AWARE INSIGHT HELPERS =====

Â  private generateProfileAwareTabs(
Â  Â  genre: string,
Â  Â  profile: PlayerProfile,
Â  Â  gameContext: GameContext
Â  ): EnhancedInsightTab[] {
Â  Â  const tabs: EnhancedInsightTab[] = [];
Â  Â Â 
Â  Â  // Generate tabs based on player profile and game context
Â  Â  if (profile.preferences?.includes('strategy')) {
Â  Â  Â  tabs.push({
Â  Â  Â  Â  id: 'strategy',
Â  Â  Â  Â  title: 'Strategy Guide',
Â  Â  Â  Â  instruction: 'Generate strategic advice based on the player\'s profile and current game context.',
Â  Â  Â  Â  priority: 'high',
Â  Â  Â  Â  playerFocus: [profile.playerFocus],
Â  Â  Â  Â  hintStyle: [profile.hintStyle],
Â  Â  Â  Â  isProfileSpecific: true
Â  Â  Â  });
Â  Â  }
Â  Â Â 
Â  Â  if (profile.preferences?.includes('lore')) {
Â  Â  Â  tabs.push({
Â  Â  Â  Â  id: 'lore',
Â  Â  Â  Â  title: 'Lore & Story',
Â  Â  Â  Â  instruction: 'Provide lore and story information relevant to the current game context.',
Â  Â  Â  Â  priority: 'medium',
Â  Â  Â  Â  playerFocus: [profile.playerFocus],
Â  Â  Â  Â  hintStyle: [profile.hintStyle],
Â  Â  Â  Â  isProfileSpecific: true
Â  Â  Â  });
Â  Â  }
Â  Â Â 
Â  Â  return tabs;
Â  }

Â  // ===== DATA ACCESS HELPERS =====

Â  private async getPlayerProfile(gameId: string): Promise<PlayerProfile | null> {
Â  Â  try {
Â  Â  Â  // This would need to be implemented based on the actual player profile service
Â  Â  Â  return null;
Â  Â  } catch (error) {
Â  Â  Â  console.warn('Failed to get player profile:', error);
Â  Â  Â  return null;
Â  Â  }
Â  }

Â  private async getGameContext(gameId: string): Promise<GameContext | null> {
Â  Â  try {
Â  Â  Â  // This would need to be implemented based on the actual game context service
Â  Â  Â  return null;
Â  Â  } catch (error) {
Â  Â  Â  console.warn('Failed to get game context:', error);
Â  Â  Â  return null;
Â  Â  }
Â  }

Â  // ===== PUBLIC API =====

Â  updateConfig(config: Partial<AIConfig>): void {
Â  Â  this.config = { ...this.config, ...config };
Â  }

Â  getConfig(): AIConfig {
Â  Â  return { ...this.config };
Â  }

Â  clearCache(): void {
Â  Â  this.insightCache.clear();
Â  Â  this.usedPrompts.clear();
Â  Â  this.saveInsightCache();
Â  Â  this.saveUsedPrompts();
Â  }

Â  // ===== PROGRESSIVE UPDATES =====

Â  private progressiveUpdates: Map<string, Record<string, { title: string; content: string }>> = new Map();

Â  private storeProgressiveUpdates(conversationId: string, updates: Record<string, { title: string; content: string }>): void {
Â  Â  this.progressiveUpdates.set(conversationId, updates);
Â  }

  getProgressiveUpdates(conversationId: string): Record<string, { title: string; content: string }> | null {
    return this.progressiveUpdates.get(conversationId) || null;
  }

  clearProgressiveUpdates(conversationId: string): void {
    this.progressiveUpdates.delete(conversationId);
  }

Â  getCacheStats(): {
Â  Â  insightCacheSize: number;
Â  Â  usedPromptsSize: number;
Â  Â  lastCleared: number;
Â  } {
Â  Â  return {
Â  Â  Â  insightCacheSize: this.insightCache.size,
Â  Â  Â  usedPromptsSize: this.usedPrompts.size,
Â  Â  Â  lastCleared: Date.now()
Â  Â  };
Â  }

Â  // ===== LONG-TERM MEMORY INTEGRATION =====

  // NEW: Get system instruction with long-term memory context
  private async getLongTermAwareSystemInstruction(
    conversation: Conversation,
    hasImages: boolean
  ): Promise<string> {
    // âœ… PERFORMANCE FIX: Parallel context fetching instead of sequential
    const [
      baseInstruction,
      longTermContext,
      screenshotTimelineContext,
      gameSpecificTimelineContext
    ] = await Promise.all([
      this.getSystemInstruction(conversation, hasImages),
      Promise.resolve(longTermMemoryService.getLongTermContext(conversation.id)),
      Promise.resolve(screenshotTimelineService.getTimelineContext(conversation.id)),
      Promise.resolve(conversation.title && conversation.title !== 'Everything Else' ?
        screenshotTimelineService.getGameSpecificTimelineContext(conversation.id, conversation.title) : '')
    ]);
Â  Â Â 
Â  Â  // NEW: Get insight tab context to prevent repetition
Â  Â  const insightTabContext = this.getInsightTabContext(conversation);
Â  Â Â 
Â  Â  // NEW: Get context summarization
Â  Â  const contextSummaryContext = this.getContextSummaryContext(conversation.id);
Â  Â Â 
Â  Â  // NEW: Get completed tasks context
Â  Â  const completedTasksContext = await this.getCompletedTasksContext(conversation.id);
Â  Â Â 
Â  Â  const longTermAwareContext = `
Â Â 
**LONG-TERM MEMORY PROTOCOL:**
- This is a long-term gaming session that may span days, weeks, or months
- User may return after extended breaks (days/weeks) - maintain full context
- Build upon ALL previous interactions, not just recent ones
- Reference progress made across the entire session history
- Provide continuity and progression awareness
- If user returns after a long break, acknowledge the gap and provide context

**CONTINUITY REQUIREMENTS:**
- Always reference previous progress and achievements
- Build upon insights from previous sessions
- Maintain narrative continuity across time gaps
- Provide context for where the user left off
- Suggest next steps based on entire session history

**SCREENSHOT TIMELINE AWARENESS:**
- ALL screenshots (single and multi-shot) represent linear progression over time
- Single screenshots show current game state at a specific moment
- Multi-shot screenshots show progression over 5-minute windows
- Batch uploads show progression over extended time periods
- Always analyze screenshots as part of a chronological sequence
- Reference previous screenshots when providing context
- Understand that each screenshot builds upon the previous ones

**GAME SWITCHING AWARENESS:**
- When user uploads screenshots of different games, switch to that game's timeline
- Provide context specific to the current game being discussed
- Reference previous interactions with the same game
- Understand that game switches create new conversation contexts
- Maintain awareness of which game the user is currently playing

**INSIGHT TAB CONTEXT AWARENESS:**
- Be aware of existing insight tab content to avoid repetition
- When generating new insights, build upon existing content rather than duplicating
- Focus on complementary information that adds value to existing tabs
- Reference existing insights when providing context and continuity

**CONTEXT COMPRESSION AWARENESS:**
- Be aware that conversation history may be compressed and summarized
- Use context summaries to maintain continuity with previous sessions
- Build upon summarized information rather than asking for repetition
- Maintain narrative flow across compressed context boundaries

**COMPLETED TASKS AWARENESS:**
- Be aware of tasks the player has already completed
- Use completed task information to understand player progress
- Avoid suggesting tasks the player has already completed
- Reference completed tasks when providing context and continuity

${longTermContext}

${screenshotTimelineContext}

${gameSpecificTimelineContext}

${insightTabContext}

${contextSummaryContext}

${completedTasksContext}
`;
Â  Â Â 
    return baseInstruction + longTermAwareContext;
  }

  // ===== NEW: UNIVERSAL SYSTEM INSTRUCTION (1:1 API Call Architecture) =====

  /**
   * Get comprehensive system instruction that instructs AI to perform all tasks in one call
   * This replaces multiple API calls with a single, structured response
   */
  private async getUniversalSystemInstruction(
    conversation: Conversation,
    hasImages: boolean,
    userMessage: string,
    conversationHistory: ChatMessage[]
  ): Promise<string> {
    // âœ… PERFORMANCE FIX: Parallel context fetching instead of sequential
    const [
      baseInstruction,
      completedTasksContext
    ] = await Promise.all([
      this.getSystemInstruction(conversation, hasImages),
      this.getCompletedTasksContext(conversation.id)
    ]);
    
    // Get all context sources (these are synchronous, so no need for Promise.all)
    const longTermContext = longTermMemoryService.getLongTermContext(conversation.id);
    const screenshotTimelineContext = screenshotTimelineService.getTimelineContext(conversation.id);
    const gameSpecificTimelineContext = conversation.title && conversation.title !== 'Everything Else' ?
      screenshotTimelineService.getGameSpecificTimelineContext(conversation.id, conversation.title) : '';
    const insightTabContext = this.getInsightTabContext(conversation);
    const contextSummaryContext = this.getContextSummaryContext(conversation.id);
    
    // Get user tier for Pro/Vanguard features
    const userTier = await unifiedUsageService.getTier();
    const isProUser = userTier === 'pro' || userTier === 'vanguard_pro';
    
    const universalInstruction = `

**CRITICAL OUTPUT REQUIREMENT:**
Your ENTIRE output MUST be a single, valid JSON object that strictly adheres to the following schema. Do not include any text, markdown, or code fences before or after the JSON object.

**OUTPUT SCHEMA:**
{
  "narrativeResponse": "string", // Your well-formatted, markdown chat response for the user. This is what they will read.
  "suggestedTasks": "DetectedTask[]", // An array of 2-3 actionable tasks based on the conversation. Use the rules from the PLAYER HISTORY section to make them relevant and non-repetitive. If no tasks are relevant, return an empty array [].
  "progressiveInsightUpdates": "InsightUpdate[]", // Analyze the conversation. If it provides new information that should update an existing insight tab (like 'story_so_far' or 'characters'), provide the updated content here. Only include tabs that need updating. If none, return an empty array [].
  "stateUpdateTags": "string[]", // Analyze the user's message for key game events. If they mention completing an objective, include "OBJECTIVE_COMPLETE: true". If they mention defeating a boss, include "TRIUMPH: <boss_name>".
  "followUpPrompts": "string[]", // Generate 3-4 engaging follow-up questions that are DIRECTLY RELATED to the content you just provided in your narrativeResponse. These should be specific, contextual questions that build upon the information you shared, not generic gaming questions. For example, if you discussed a specific game's mechanics, ask about related mechanics or strategies. If you mentioned specific games, ask about similar games or related topics. Make them feel like natural next steps in the conversation.
  "gamePillData": { // Available for all users when game pill should be created
    "shouldCreate": boolean, // Set to true when user needs help with a specific game and no game pill exists yet
    "gameName": string, // Extract from user message or identify from screenshot (e.g., "Elden Ring", "The Witcher 3")
    "genre": string, // Game genre (e.g., "Action RPG", "Strategy", "Platformer")
    "wikiContent": { "tabId": "content" } // Multiple insight tabs with comprehensive game information
  },
  "taskCompletionPrompt": { // Only if user has active tasks
    "tasks": [],
    "prompt": string,
    "category": string
  }
}

**TASK GENERATION RULES:**
1. **ACTIONABLE**: Generate 2-3 specific, actionable tasks
2. **PROGRESS-APPROPRIATE**: Tasks should match current game progress
3. **CONTEXT-AWARE**: Use player history to avoid repeating tasks
4. **NO SPOILERS**: Only tasks accessible at current progress level
5. **VARIETY**: Mix of quests, exploration, items, and character interactions
6. **INSIGHT-AWARE**: Don't suggest tasks already covered in insights
7. **COMPLETION-AWARE**: NEVER suggest tasks the player has already completed
8. **PROGRESSIVE**: Build upon completed tasks to suggest next logical steps

**FOLLOW-UP PROMPT GENERATION RULES:**
1. **CONTENT-SPECIFIC**: Questions must be directly related to the information you provided in your narrativeResponse
2. **CONTEXTUAL**: Build upon specific games, mechanics, or topics you mentioned
3. **NATURAL PROGRESSION**: Feel like logical next steps in the conversation
4. **ENGAGING**: Ask questions that encourage deeper exploration of the topic
5. **SPECIFIC**: Avoid generic gaming questions - be specific to the content discussed
6. **VARIED**: Mix different types of questions (how-to, comparisons, recommendations, etc.)
7. **RELEVANT**: Only ask questions that make sense given the conversation context
8. **ACTIONABLE**: Questions should lead to useful follow-up responses

**PROGRESSIVE INSIGHT UPDATES:**
- Only update insight tabs if the conversation provides NEW information
- Focus on tabs like 'story_so_far', 'characters', 'locations', 'items'
- Provide updated content that incorporates the new information
- Don't update tabs that don't need changes

**GAME PILL CREATION (All Users):**
**CURRENT CONVERSATION: "${conversation.id}"** ${conversation.id === 'everything-else' ? '(General chat - can create game pills)' : '(Game-specific conversation - do NOT create game pills)'}

- SET shouldCreate: true WHEN:
  â€¢ User asks for help with a specific game AND conversation.id is "everything-else"
  â€¢ User uploads a screenshot of a game AND asks for help AND conversation.id is "everything-else"
  â€¢ User uploads a screenshot of a game WITHOUT text AND conversation.id is "everything-else" (AI should identify game from image)
  â€¢ User mentions they're playing a specific game and need assistance AND conversation.id is "everything-else"
  â€¢ User asks questions like "help with [game name]" or "stuck in [game name]" AND conversation.id is "everything-else"
- SET shouldCreate: false WHEN:
  â€¢ conversation.id is NOT "everything-else" (game pill already exists for this game)
  â€¢ User is asking general gaming questions without mentioning a specific game
  â€¢ User is asking about non-gaming topics
- When shouldCreate is true:
  â€¢ Extract the game name from user's message OR identify it from the uploaded screenshot
  â€¢ If only screenshot is provided (no text), analyze the image to determine the game name
  â€¢ Determine the game genre (RPG, Action, Strategy, etc.)
  â€¢ Generate comprehensive wiki-like content for multiple insight tabs
  â€¢ Include tabs like: story_so_far, characters, locations, items, tips_and_tricks
  â€¢ NOTE: Free users get basic tabs, Pro/Vanguard users get rich content

**CONTEXT SOURCES:**
${longTermContext}
${screenshotTimelineContext}
${gameSpecificTimelineContext}
${insightTabContext}
${contextSummaryContext}
${completedTasksContext}

**USER TIER: ${userTier}** ${isProUser ? '(Pro/Vanguard features enabled)' : '(Free tier - basic features only)'}

Analyze the user's query and the full context provided, then perform all the requested tasks and populate the JSON object accordingly.
`;

    return baseInstruction + universalInstruction;
  }

  /**
   * Handle game pill creation for all users
   */
  private async handleGamePillCreation(
    conversation: Conversation,
    gamePillData: { shouldCreate: boolean; gameName: string; genre: string; wikiContent: Record<string, string> },
    signal?: AbortSignal
  ): Promise<void> {
    try {
      if (!gamePillData.shouldCreate) return;

      console.log(`ğŸ® Creating game pill for ${gamePillData.gameName} (${gamePillData.genre})`);
      
      // Update conversation with game pill data
      if (conversation.insights) {
        Object.assign(conversation.insights, gamePillData.wikiContent as unknown as Record<string, Insight>);
      } else {
        conversation.insights = gamePillData.wikiContent as unknown as Record<string, Insight>;
      }

      // Update conversation genre if not set
      if (!conversation.genre) {
        conversation.genre = gamePillData.genre;
      }

      console.log(`âœ… Game pill created with ${Object.keys(gamePillData.wikiContent).length} insight tabs`);
    } catch (error) {
      console.warn('Failed to create game pill:', error);
    }
  }

Â  // NEW: Get insight tab context to prevent repetition
Â  public getInsightTabContext(conversation: Conversation): string {
Â  Â  if (!conversation.insights || Object.keys(conversation.insights).length === 0) {
Â  Â  Â  return '';
Â  Â  }

Â  Â  const insightTabs = Object.entries(conversation.insights);
Â  Â  let contextString = `
[META_INSIGHT_TABS_CONTEXT: The following insight tabs already exist with content - DO NOT regenerate similar content for these tabs:`;

Â  Â  insightTabs.forEach(([tabId, insight]) => {
Â  Â  Â  if (insight && insight.content) {
Â  Â  Â  Â  // Truncate content to avoid context bloat
Â  Â  Â  Â  const truncatedContent = insight.content.length > 150Â 
Â  Â  Â  Â  Â  ? insight.content.substring(0, 150) + '...'Â 
Â  Â  Â  Â  Â  : insight.content;
Â  Â  Â  Â  contextString += `
- ${tabId}: "${truncatedContent}"`;
Â  Â  Â  }
Â  Â  });

Â  Â  contextString += `
When generating new insights, avoid duplicating content from these existing tabs and focus on new, complementary information.]
`;

Â  Â  return contextString;
Â  }

  // NEW: Get context summary context
  private getContextSummaryContext(conversationId: string): string {
    try {
      // Use dynamic import instead of require for browser compatibility
      import('./contextSummarizationService').then(({ contextSummarizationService }) => {
        return contextSummarizationService.getContextSummaryForAI(conversationId);
      }).catch(() => {
        return '';
      });
      return ''; // Return empty string immediately for now
    } catch (error) {
      console.warn('Context summarization service not available:', error);
      return '';
    }
  }

Â  // NEW: Get completed tasks context for AI awareness
Â  private async getCompletedTasksContext(conversationId: string): Promise<string> {
    try {
      // Using static import instead of dynamic import for Firebase hosting compatibility
      const tasks = await otakuDiaryService.getTasks(conversationId);
      const completedTasks = tasks.filter(task => task.status === 'completed');
Â  Â  Â Â 
Â  Â  Â  if (completedTasks.length === 0) {
Â  Â  Â  Â  return '';
Â  Â  Â  }

Â  Â  Â  let contextString = `[META_COMPLETED_TASKS: Player has completed the following tasks - use this information to understand their progress and avoid suggesting similar tasks:\n`;
Â  Â  Â Â 
Â  Â  Â  completedTasks.forEach((task, index) => {
Â  Â  Â  Â  const taskAge = Date.now() - (task.completedAt || task.createdAt);
Â  Â  Â  Â  const ageInDays = Math.floor(taskAge / (24 * 60 * 60 * 1000));
Â  Â  Â  Â Â 
Â  Â  Â  Â  contextString += `${index + 1}. ${task.title} (${task.category}) - completed ${ageInDays} days ago\n`;
Â  Â  Â  Â  if (task.description) {
Â  Â  Â  Â  Â  contextString += `Â  Â Details: ${task.description.substring(0, 100)}${task.description.length > 100 ? '...' : ''}\n`;
Â  Â  Â  Â  }
Â  Â  Â  });
Â  Â  Â Â 
Â  Â  Â  contextString += `Use this information to provide context-aware responses and avoid suggesting tasks the player has already completed.]\n`;
Â  Â  Â Â 
Â  Â  Â  return contextString;
Â  Â  } catch (error) {
Â  Â  Â  console.warn('Failed to get completed tasks context:', error);
Â  Â  Â  return '';
Â  Â  }
Â  }

Â  // NEW: Generate AI suggested tasks based on conversation context
Â  public async generateSuggestedTasks(
Â  Â  conversation: Conversation,
Â  Â  userQuery: string,
Â  Â  aiResponse: string,
Â  Â  signal?: AbortSignal
Â  ): Promise<DetectedTask[]> {
Â  Â  try {
Â  Â  Â  // Get context from various sources
Â  Â  Â  const longTermContext = longTermMemoryService.getLongTermContext(conversation.id);
Â  Â  Â  const screenshotTimelineContext = screenshotTimelineService.getTimelineContext(conversation.id);
Â  Â  Â  const insightTabContext = this.getInsightTabContext(conversation);
Â  Â  Â  const completedTasksContext = await this.getCompletedTasksContext(conversation.id);
Â  Â  Â Â 
Â  Â  Â  const systemInstruction = `
You are Otakon, a master game analyst. Generate actionable tasks based on the user's query, your response, and the player's history.

**CONTEXT:**
- Game: ${conversation.title}
- User Query: "${userQuery}"
- AI Response: "${aiResponse}"
- Current Progress: ${conversation.progress || 0}%

**PLAYER HISTORY:**
${longTermContext}
${screenshotTimelineContext}
${insightTabContext}
${completedTasksContext}

**TASK GENERATION RULES:**
1. **ACTIONABLE**: Generate 2-3 specific, actionable tasks
2. **PROGRESS-APPROPRIATE**: Tasks should match current game progress
3. **CONTEXT-AWARE**: Use player history to avoid repeating tasks
4. **NO SPOILERS**: Only tasks accessible at current progress level
5. **VARIETY**: Mix of quests, exploration, items, and character interactions
6. **INSIGHT-AWARE**: Don't suggest tasks already covered in insights
7. **COMPLETION-AWARE**: NEVER suggest tasks the player has already completed
8. **PROGRESSIVE**: Build upon completed tasks to suggest next logical steps

**OUTPUT FORMAT:**
Return a JSON array of tasks with:
- title: Short, clear task title
- description: Detailed task description
- category: quest|boss|exploration|item|character|custom
- confidence: 0.0-1.0
- source: "context_aware_ai"

**EXAMPLE:**
[
Â  {
Â  Â  "title": "Find the Hidden Shrine",
Â  Â  "description": "Explore the Whispering Caverns to locate the ancient shrine mentioned in the lore. Look for a waterfall that hides more than just a damp cave wall.",
Â  Â  "category": "exploration",
Â  Â  "confidence": 0.8,
Â  Â  "source": "context_aware_ai"
Â  }
]
`;

Â  Â  Â  const response = await this.generateContent({
Â  Â  Â  Â  model: 'gemini-2.5-flash',
Â  Â  Â  Â  contents: `Generate actionable tasks based on this conversation: "${userQuery}" -> "${aiResponse}"`,
Â  Â  Â  Â  config: { systemInstruction },
Â  Â  Â  Â  signal
Â  Â  Â  });

Â  Â  Â  // Parse and return tasks
Â  Â  Â  return this.parseSuggestedTasks(response);
Â  Â  } catch (error) {
Â  Â  Â  console.error('Failed to generate suggested tasks:', error);
Â  Â  Â  return [];
Â  Â  }
Â  }

Â  // NEW: Parse suggested tasks from AI response
Â  private parseSuggestedTasks(response: string): DetectedTask[] {
Â  Â  try {
Â  Â  Â  // Try to extract JSON from response
Â  Â  Â  const jsonMatch = response.match(/\[[\s\S]*\]/);
Â  Â  Â  if (jsonMatch) {
Â  Â  Â  Â  const tasks = JSON.parse(jsonMatch[0]);
Â  Â  Â  Â  return tasks.filter((task: any) =>Â 
Â  Â  Â  Â  Â  task.title &&Â 
Â  Â  Â  Â  Â  task.description &&Â 
Â  Â  Â  Â  Â  task.category &&Â 
Â  Â  Â  Â  Â  typeof task.confidence === 'number'
Â  Â  Â  Â  );
Â  Â  Â  }
Â  Â  Â Â 
Â  Â  Â  // Fallback: create tasks from response text
Â  Â  Â  return this.createFallbackTasks(response);
Â  Â  } catch (error) {
Â  Â  Â  console.warn('Failed to parse suggested tasks, using fallback:', error);
Â  Â  Â  return this.createFallbackTasks(response);
Â  Â  }
Â  }

Â  // NEW: Create fallback tasks from response text
Â  private createFallbackTasks(response: string): DetectedTask[] {
Â  Â  const tasks: DetectedTask[] = [];
Â  Â Â 
Â  Â  // Simple pattern matching to extract tasks
Â  Â  const lines = response.split('\n').filter(line => line.trim());
Â  Â Â 
Â  Â  for (const line of lines) {
Â  Â  Â  if (line.includes('â€¢') || line.includes('-') || line.includes('*')) {
Â  Â  Â  Â  const cleanLine = line.replace(/^[â€¢\-*]\s*/, '').trim();
Â  Â  Â  Â  if (cleanLine.length > 10) {
Â  Â  Â  Â  Â  tasks.push({
Â  Â  Â  Â  Â  Â  title: cleanLine.substring(0, 50) + (cleanLine.length > 50 ? '...' : ''),
Â  Â  Â  Â  Â  Â  description: cleanLine,
Â  Â  Â  Â  Â  Â  category: 'custom',
Â  Â  Â  Â  Â  Â  confidence: 0.6,
Â  Â  Â  Â  Â  Â  source: 'fallback_parsing'
Â  Â  Â  Â  Â  });
Â  Â  Â  Â  }
Â  Â  Â  }
Â  Â  }
Â  Â Â 
Â  Â  return tasks.slice(0, 3); // Limit to 3 tasks
Â  }

Â  // ===== CLEANUP =====

Â  // ===== ENHANCED INSIGHT METHODS (from enhancedInsightService) =====
Â Â 
Â  getTabsForGenre(genre: string): EnhancedInsightTab[] {
Â  Â  // Return default tabs for the genre
Â  Â  const defaultTabs = insightTabsConfig[genre] || insightTabsConfig.default;
Â  Â  return defaultTabs.map(tab => ({
Â  Â  Â  ...tab,
Â  Â  Â  lastUpdated: Date.now(),
Â  Â  Â  priority: 'medium' as const,
Â  Â  Â  playerFocus: [],
Â  Â  Â  hintStyle: [],
Â  Â  Â  isProfileSpecific: false
Â  Â  }));
Â  }

Â  prioritizeTabsForProfile(tabs: EnhancedInsightTab[], profile: PlayerProfile): EnhancedInsightTab[] {
Â  Â  // Simple prioritization based on profile preferences
Â  Â  return tabs.sort((a, b) => {
Â  Â  Â  const aPriority = this.getTabPriorityForProfile(a, profile);
Â  Â  Â  const bPriority = this.getTabPriorityForProfile(b, profile);
Â  Â  Â  return bPriority - aPriority;
Â  Â  });
Â  }

Â  private getTabPriorityForProfile(tab: EnhancedInsightTab, profile: PlayerProfile): number {
Â  Â  let priority = tab.priority === 'high' ? 3 : tab.priority === 'medium' ? 2 : 1;
Â  Â Â 
Â  Â  // Boost priority for profile-specific tabs
Â  Â  if (tab.isProfileSpecific) {
Â  Â  Â  priority += 1;
Â  Â  }
Â  Â Â 
Â  Â  return priority;
Â  }

Â  needsContentGeneration(tabs: EnhancedInsightTab[]): boolean {
Â  Â  return tabs.some(tab => !tab.content || tab.content === 'Content will be generated when you ask for help.');
Â  }

Â  getTabsForProModel(tabs: EnhancedInsightTab[], userTier: string): EnhancedInsightTab[] {
Â  Â  if (userTier === 'free') return [];
Â  Â  return tabs.filter(tab => tab.generationModel === 'pro');
Â  }

Â  generateInsightsForNewGamePill(
Â  Â  gameName: string,
Â  Â  genre: string,
Â  Â  progress: number,
Â  Â  userTier: string,
Â  Â  profile: Promise<PlayerProfile>
Â  ): Promise<Record<string, InsightResult>> {
Â  Â  return new Promise((resolve) => {
Â  Â  Â  // Return empty insights for now - will be generated on demand
Â  Â  Â  resolve({});
Â  Â  });
Â  }

Â  updateInsightsForUserQuery(
Â  Â  gameName: string,
Â  Â  genre: string,
Â  Â  progress: number,
Â  Â  userQuery: string,
Â  Â  userTier: string,
Â  Â  profile: Promise<PlayerProfile>
Â  ): Promise<Record<string, InsightResult>> {
Â  Â  return new Promise((resolve) => {
Â  Â  Â  // Return empty insights for now - will be generated on demand
Â  Â  Â  resolve({});
Â  Â  });
Â  }

Â  // ===== PROACTIVE INSIGHT METHODS (from proactiveInsightService) =====
Â Â 
Â  getProactiveInsights(): any[] {
Â  Â  // Return empty array for now
Â  Â  return [];
Â  }

Â  markInsightAsRead(insightId: string): Promise<void> {
Â  Â  return Promise.resolve();
Â  }

Â  deleteInsight(insightId: string): Promise<void> {
Â  Â  return Promise.resolve();
Â  }

Â  // ===== HELPER METHODS FROM GEMINI SERVICE =====

Â  /**
Â  Â * Check and cache content using universal cache service (from geminiService)
Â  Â */
Â  private async checkAndCacheContent(
Â  Â  query: string,
Â  Â  contentType: CacheQuery['contentType'],
Â  Â  gameName?: string,
Â  Â  genre?: string
Â  ): Promise<{ found: boolean; content?: string; reason?: string }> {
Â  Â  try {
Â  Â  Â  const userTier = await unifiedUsageService.getTier();
Â  Â  Â Â 
Â  Â  Â  const cacheQuery: CacheQuery = {
Â  Â  Â  Â  query,
Â  Â  Â  Â  contentType,
Â  Â  Â  Â  gameName,
Â  Â  Â  Â  genre,
Â  Â  Â  Â  userTier
Â  Â  Â  };
Â  Â  Â Â 
Â  Â  Â  const cacheResult = await universalContentCacheService.getCachedContent(cacheQuery);
Â  Â  Â Â 
Â  Â  Â  if (cacheResult.found && cacheResult.content) {
Â  Â  Â  Â  console.log(`ğŸ¯ Found cached ${contentType} content: ${query.substring(0, 50)}...`);
Â  Â  Â  Â  return {
Â  Â  Â  Â  Â  found: true,
Â  Â  Â  Â  Â  content: cacheResult.content.content,
Â  Â  Â  Â  Â  reason: cacheResult.reason
Â  Â  Â  Â  };
Â  Â  Â  }
Â  Â  Â Â 
Â  Â  Â  return { found: false };
Â  Â  } catch (error) {
Â  Â  Â  console.warn('Failed to check universal cache:', error);
Â  Â  Â  return { found: false };
Â  Â  }
Â  }

Â  /**
Â  Â * Cache content after AI generation (from geminiService)
Â  Â */
Â  private async cacheGeneratedContent(
Â  Â  query: string,
Â  Â  content: string,
Â  Â  contentType: CacheQuery['contentType'],
Â  Â  gameName?: string,
Â  Â  genre?: string,
Â  Â  model: string = 'gemini-2.5-flash',
Â  Â  tokens: number = 0,
Â  Â  cost: number = 0
Â  ): Promise<void> {
Â  Â  try {
Â  Â  Â  const userTier = await unifiedUsageService.getTier();
Â  Â  Â Â 
Â  Â  Â  const cacheQuery: CacheQuery = {
Â  Â  Â  Â  query,
Â  Â  Â  Â  contentType,
Â  Â  Â  Â  gameName,
Â  Â  Â  Â  genre,
Â  Â  Â  Â  userTier
Â  Â  Â  };
Â  Â  Â Â 
Â  Â  Â  await universalContentCacheService.cacheContent(cacheQuery, content, {
Â  Â  Â  Â  model,
Â  Â  Â  Â  tokens,
Â  Â  Â  Â  cost,
Â  Â  Â  Â  tags: [gameName, genre].filter(Boolean) as string[]
Â  Â  Â  });
Â  Â  Â Â 
Â  Â  Â  console.log(`ğŸ’¾ Cached ${contentType} content for future use`);
Â  Â  } catch (error) {
Â  Â  Â  console.warn('Failed to cache generated content:', error);
Â  Â  }
Â  }

Â  /**
Â  Â * Track AI response context for learning (from geminiService)
Â  Â */
Â  private async trackAIResponse(
Â  Â  conversation: Conversation,
Â  Â  userMessage: string,
Â  Â  aiResponse: string,
Â  Â  hasImages: boolean = false,
Â  Â  validationIssues: string[] = []
Â  ): Promise<void> {
Â  Â  try {
Â  Â  Â  const userId = authService.getAuthState().user?.id;
Â  Â  Â  if (!userId) return;

Â  Â  Â  // Analyze AI response context
Â  Â  Â  const aiContext = {
Â  Â  Â  Â  response_length: aiResponse.length,
Â  Â  Â  Â  has_code: aiResponse.includes('```') || aiResponse.includes('`'),
Â  Â  Â  Â  has_images: hasImages,
Â  Â  Â  Â  response_type: hasImages ? 'image_analysis' : 'text_response',
Â  Â  Â  Â  conversation_id: conversation.id,
Â  Â  Â  Â  game_genre: conversation.genre,
Â  Â  Â  Â  user_progress: conversation.progress,
Â  Â  Â  Â  validation_issues: validationIssues,
Â  Â  Â  Â  has_validation_issues: validationIssues.length > 0
Â  Â  Â  };

Â  Â  Â  // Get user context
Â  Â  Â  const userContext = {
Â  Â  Â  Â  user_tier: unifiedUsageService.getTier(),
Â  Â  Â  Â  game_genre: conversation.genre,
Â  Â  Â  Â  user_progress: conversation.progress,
Â  Â  Â  Â  conversation_title: conversation.title
Â  Â  Â  };

Â  Â  Â  // Store for potential feedback analysis
Â  Â  Â  await aiContextService.storeUserContext('behavior', {
Â  Â  Â  Â  last_ai_response: aiContext,
Â  Â  Â  Â  last_user_message: userMessage,
Â  Â  Â  Â  timestamp: Date.now()
Â  Â  Â  });

Â  Â  Â  // Track user behavior
Â  Â  Â  await aiContextService.trackUserBehavior(
Â  Â  Â  Â  'ai_interaction',
Â  Â  Â  Â  {
Â  Â  Â  Â  Â  message_type: hasImages ? 'image' : 'text',
Â  Â  Â  Â  Â  conversation_id: conversation.id,
Â  Â  Â  Â  Â  game_title: conversation.title
Â  Â  Â  Â  },
Â  Â  Â  Â  {
Â  Â  Â  Â  Â  ai_response_length: aiResponse.length,
Â  Â  Â  Â  Â  has_images: hasImages
Â  Â  Â  Â  }
Â  Â  Â  );
Â  Â  } catch (error) {
Â  Â  Â  console.warn('Failed to track AI response:', error);
Â  Â  }
Â  }

Â  /**
Â  Â * Progress detection from AI responses (from geminiService)
Â  Â */
Â  async detectProgressFromResponse(
Â  Â  conversation: Conversation,
Â  Â  userMessage: string,
Â  Â  aiResponse: string,
Â  Â  userId: string
Â  ): Promise<void> {
Â  Â  console.log('ğŸ¤– Gemini AI: Analyzing message for progress detection', {
Â  Â  Â  conversationTitle: conversation.title,
Â  Â  Â  userMessage,
Â  Â  Â  userId
Â  Â  });
Â  Â Â 
Â  Â  try {
Â  Â  Â  // Simple progress detection based on common gaming phrases
Â  Â  Â  const progressIndicators = [
Â  Â  Â  Â  { phrase: 'defeated', eventType: 'boss_defeat', confidence: 0.7 },
Â  Â  Â  Â  { phrase: 'completed', eventType: 'quest_completion', confidence: 0.8 },
Â  Â  Â  Â  { phrase: 'found', eventType: 'item_acquisition', confidence: 0.6 },
Â  Â  Â  Â  { phrase: 'discovered', eventType: 'location_discovery', confidence: 0.7 },
Â  Â  Â  Â  { phrase: 'reached', eventType: 'story_progression', confidence: 0.6 },
Â  Â  Â  Â  { phrase: 'unlocked', eventType: 'story_progression', confidence: 0.8 }
Â  Â  Â  ];

Â  Â  Â  for (const indicator of progressIndicators) {
Â  Â  Â  Â  if (userMessage.toLowerCase().includes(indicator.phrase.toLowerCase())) {
Â  Â  Â  Â  Â  // Extract game ID from conversation title or context
Â  Â  Â  Â  Â  const gameId = conversation.title.toLowerCase().includes('elden ring') ? 'elden_ring' :
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  conversation.title.toLowerCase().includes('cyberpunk') ? 'cyberpunk_2077' :
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  conversation.title.toLowerCase().includes('zelda') ? 'zelda_tears_kingdom' :
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  conversation.title.toLowerCase().includes('baldurs') ? 'baldurs_gate_3' : 'unknown';

Â  Â  Â  Â  Â  if (gameId !== 'unknown') {
Â  Â  Â  Â  Â  Â  // Using static import instead of dynamic import for Firebase hosting compatibility
Â  Â  Â  Â  Â  Â  await progressTrackingService.updateProgressForAnyGame(
Â  Â  Â  Â  Â  Â  Â  userId,
Â  Â  Â  Â  Â  Â  Â  gameId,
Â  Â  Â  Â  Â  Â  Â  indicator.eventType,
Â  Â  Â  Â  Â  Â  Â  `AI-detected ${indicator.eventType} from user message`,
Â  Â  Â  Â  Â  Â  Â  3, // Default progress level
Â  Â  Â  Â  Â  Â  Â  'base_game',
Â  Â  Â  Â  Â  Â  Â  indicator.confidence,
Â  Â  Â  Â  Â  Â  Â  'Progress detected from user message',
Â  Â  Â  Â  Â  Â  Â  [userMessage]
Â  Â  Â  Â  Â  Â  );
Â  Â  Â  Â  Â  }
Â  Â  Â  Â  Â  break; // Only detect one progress event per message
Â  Â  Â  Â  }
Â  Â  Â  }
Â  Â  } catch (error) {
Â  Â  Â  console.warn('Progress detection failed:', error);
Â  Â  }
Â  }

Â  /**
Â  Â * Handle success after API call (from geminiService)
Â  Â */
Â  private async handleSuccess(): Promise<void> {
Â  Â  try {
Â  Â  Â  // Clear cooldown in Supabase
Â  Â  Â  await supabaseDataService.setAppCache('geminiCooldown', null, new Date(0).toISOString());
Â  Â  Â Â 
Â  Â  Â  // Also clear localStorage as backup
Â  Â  Â  const cooldownEnd = localStorage.getItem(COOLDOWN_KEY);
Â  Â  Â  if (cooldownEnd) {
Â  Â  Â  Â  console.log("API call successful, clearing cooldown.");
Â  Â  Â  Â  localStorage.removeItem(COOLDOWN_KEY);
Â  Â  Â  }
Â  Â  } catch (error) {
Â  Â  Â  console.warn('Failed to clear cooldown in Supabase, using localStorage only:', error);
Â  Â  Â Â 
Â  Â  Â  // Fallback to localStorage only
Â  Â  Â  const cooldownEnd = localStorage.getItem(COOLDOWN_KEY);
Â  Â  Â  if (cooldownEnd) {
Â  Â  Â  Â  console.log("API call successful, clearing cooldown (localStorage fallback).");
Â  Â  Â  Â  localStorage.removeItem(COOLDOWN_KEY);
Â  Â  Â  }
Â  Â  }
Â  }

Â  /**
Â  Â * Handle errors from API calls (from geminiService)
Â  Â */
Â  private handleError(error: any, onError: (error: string) => void): void {
Â  Â  console.error("Gemini Service Error Details:", error);

Â  Â  const errorMessage = error?.message || error.toString();
Â  Â  if (error?.httpError?.status === 0 || errorMessage.includes('status code: 0')) {
Â  Â  Â  onError("I couldn't reach the network. This can happen if the screen is locked and the device is saving power. Waking the screen and trying again usually helps.");
Â  Â  Â  return;
Â  Â  }
Â  Â Â 
Â  Â  if (isQuotaError(error)) {
Â  Â  Â  onError("QUOTA_EXCEEDED");
Â  Â  Â  return;
Â  Â  }

Â  Â  // Handle 503 Service Unavailable / Model Overloaded errors
Â  Â  if (error?.httpError?.status === 503 || errorMessage.includes('503') ||Â 
Â  Â  Â  Â  errorMessage.includes('overloaded') || errorMessage.includes('UNAVAILABLE')) {
Â  Â  Â  onError("The AI is currently experiencing high traffic and is temporarily unavailable. Please wait a moment and try again. This usually resolves within a few minutes.");
Â  Â  Â  return;
Â  Â  }

Â  Â  let message = "An unknown error occurred while contacting the AI.";
Â  Â  if (error instanceof Error) {
Â  Â  Â  if (error.message.includes('API key not valid')) {
Â  Â  Â  Â  message = "Error: The provided API Key is not valid. Please check your configuration.";
Â  Â  Â  } else {
Â  Â  Â  Â  try {
Â  Â  Â  Â  Â  // The error message from Gemini is often a JSON string itself
Â  Â  Â  Â  Â  const parsedError = JSON.parse(error.message);
Â  Â  Â  Â  Â  if (parsedError.error && parsedError.error.message) {
Â  Â  Â  Â  Â  Â  // Handle 503 errors from parsed JSON
Â  Â  Â  Â  Â  Â  if (parsedError.error.code === 503 || parsedError.error.status === 'UNAVAILABLE') {
Â  Â  Â  Â  Â  Â  Â  message = "The AI is currently experiencing high traffic and is temporarily unavailable. Please wait a moment and try again. This usually resolves within a few minutes.";
Â  Â  Â  Â  Â  Â  } else {
Â  Â  Â  Â  Â  Â  Â  message = `Error: ${parsedError.error.message}`;
Â  Â  Â  Â  Â  Â  }
Â  Â  Â  Â  Â  }
Â  Â  Â  Â  } catch (e) {
Â  Â  Â  Â  Â  // Fallback if the message is not JSON
Â  Â  Â  Â  Â  message = `Error: ${error.message}`;
Â  Â  Â  Â  }
Â  Â  Â  }
Â  Â  }
Â  Â  onError(message);
Â  }

Â  /**
Â  Â * Get or create chat session (from geminiService)
Â  Â */
Â  private async getOrCreateChat(
Â  Â  conversation: Conversation,Â 
Â  Â  hasImages: boolean,Â 
Â  Â  model: GeminiModel,Â 
Â  Â  history: ChatMessage[] = []
Â  ): Promise<Chat> {
Â  Â  const conversationId = conversation.id;
Â  Â  const existingSession = this.chatSessions[conversationId];
Â  Â  if (existingSession && existingSession.model === model) {
Â  Â  Â  return existingSession.chat;
Â  Â  }

Â  Â  if (existingSession && existingSession.model !== model) {
Â  Â  Â  console.log(`Model switch for ${conversationId}. Recreating chat from ${existingSession.model} to ${model}.`);
Â  Â  Â  delete this.chatSessions[conversationId];
Â  Â  }
Â  Â Â 
Â  Â  console.log(`Creating new chat session for ${conversationId} with model ${model} and ${history.length} history messages.`);
Â  Â  const geminiHistory = await this.mapMessagesToGeminiContent(history);
Â  Â Â 
Â  Â  const systemInstruction = await this.getSystemInstruction(conversation, hasImages);
Â  Â Â 
Â  Â  this.ensureAIInitialized();
Â  Â  if (!this.ai) {
Â  Â  Â  throw new Error('AI service not available: No API key provided');
Â  Â  }

Â  Â  const newChat = this.ai.chats.create({
Â  Â  Â  model,
Â  Â  Â  history: geminiHistory,
Â  Â  Â  config: {
Â  Â  Â  Â  systemInstruction,
Â  Â  Â  Â  tools: [{ googleSearch: {} }]
Â  Â  Â  }
Â  Â  });
Â  Â  this.chatSessions[conversationId] = { chat: newChat, model };
Â  Â  return newChat;
Â  }

Â  /**
Â  Â * Map messages to Gemini content format (from geminiService)
Â  Â */
Â  private async mapMessagesToGeminiContent(messages: ChatMessage[]): Promise<Content[]> {
Â  Â  const history: Content[] = [];
Â  Â Â 
Â  Â  // NEW: Apply context compression and summarization
Â  Â  let processedMessages = messages;
Â  Â Â 
Â  Â  // Import context summarization service dynamically to avoid circular dependencies
Â  Â  let contextSummarizationService: any = null;
Â  Â  try {
Â  Â  Â  // Use dynamic import instead of require for better compatibility
Â  Â  Â  const module = await import('./contextSummarizationService');
Â  Â  Â  contextSummarizationService = module.contextSummarizationService;
Â  Â  } catch (error) {
Â  Â  Â  console.warn('Context summarization service not available:', error);
Â  Â  }
Â  Â Â 
Â  Â  // Apply compression if we have many messages
Â  Â  if (messages.length > 20 && contextSummarizationService) {
Â  Â  Â  const compressionResult = contextSummarizationService.compressConversationHistory(
Â  Â  Â  Â  'current-conversation', // We'll pass the actual conversation ID later
Â  Â  Â  Â  messages,
Â  Â  Â  Â  20
Â  Â  Â  );
Â  Â  Â Â 
Â  Â  Â  processedMessages = compressionResult.compressedMessages;
Â  Â  Â Â 
Â  Â  Â  // Add summary as a system message if we have one
Â  Â  Â  if (compressionResult.summary) {
Â  Â  Â  Â  history.push({
Â  Â  Â  Â  Â  role: 'model',
Â  Â  Â  Â  Â  parts: [{ text: `[CONTEXT_SUMMARY] ${compressionResult.summary.summary}` }]
Â  Â  Â  Â  });
Â  Â  Â  }
Â  Â  Â Â 
Â  Â  Â  console.log(`ğŸ“Š Context Compression: ${compressionResult.originalCount} â†’ ${compressionResult.compressedCount} messages (${Math.round(compressionResult.compressionRatio * 100)}% retained)`);
Â  Â  } else {
Â  Â  Â  // Apply simple limits if no compression
Â  Â  Â  processedMessages = messages.slice(-20);
Â  Â  }
Â  Â Â 
Â  Â  let totalImages = 0;
Â  Â  let estimatedTokens = 0;
Â  Â Â 
Â  Â  console.log(`ğŸ“Š Context Management: Processing ${processedMessages.length} messages (limited from ${messages.length})`);
Â  Â Â 
Â  Â  for (const message of processedMessages) {
Â  Â  Â  const parts: Part[] = [];
Â  Â  Â  if (message.role !== 'user' && message.role !== 'model') continue;
Â  Â  Â Â 
Â  Â  Â  // NEW: Limit images in context
Â  Â  Â  if (message.images && message.images.length > 0) {
Â  Â  Â  Â  const imagesToInclude = Math.min(message.images.length, 10 - totalImages);
Â  Â  Â  Â  if (imagesToInclude <= 0) {
Â  Â  Â  Â  Â  console.log(`ğŸ“Š Context Management: Skipping ${message.images.length} images (limit reached)`);
Â  Â  Â  Â  Â  continue;
Â  Â  Â  Â  }
Â  Â  Â  Â Â 
Â  Â  Â  Â  for (let i = 0; i < imagesToInclude; i++) {
Â  Â  Â  Â  Â  const imageUrl = message.images[i];
Â  Â  Â  Â  Â  try {
Â  Â  Â  Â  Â  Â  const [meta, base64] = imageUrl.split(',');
Â  Â  Â  Â  Â  Â  if (!meta || !base64) continue;
Â  Â  Â  Â  Â  Â  const mimeTypeMatch = meta.match(/:(.*?);/);
Â  Â  Â  Â  Â  Â  if (!mimeTypeMatch?.[1]) continue;
Â  Â  Â  Â  Â  Â  parts.push({ inlineData: { data: base64, mimeType: mimeTypeMatch[1] } });
Â  Â  Â  Â  Â  Â  totalImages++;
Â  Â  Â  Â  Â  Â  estimatedTokens += 1000; // Approximate tokens per image
Â  Â  Â  Â  Â  } catch (e) {
Â  Â  Â  Â  Â  Â  console.error("Skipping malformed image in history", e);
Â  Â  Â  Â  Â  Â  continue;
Â  Â  Â  Â  Â  }
Â  Â  Â  Â  }
Â  Â  Â  }
Â  Â  Â Â 
Â  Â  Â  if (message.text) {
Â  Â  Â  Â  // NEW: Estimate text tokens and limit if necessary
Â  Â  Â  Â  const textTokens = Math.ceil(message.text.length / 4); // Rough estimate: 4 chars = 1 token
Â  Â  Â  Â  estimatedTokens += textTokens;
Â  Â  Â  Â Â 
Â  Â  Â  Â  if (estimatedTokens > 30000) {
Â  Â  Â  Â  Â  console.log(`ğŸ“Š Context Management: Token limit reached (${estimatedTokens}), truncating text`);
Â  Â  Â  Â  Â  const remainingTokens = 30000 - (estimatedTokens - textTokens);
Â  Â  Â  Â  Â  const truncatedText = message.text.substring(0, remainingTokens * 4);
Â  Â  Â  Â  Â  parts.push({ text: truncatedText + "... [Context truncated]" });
Â  Â  Â  Â  Â  break; // Stop processing more messages
Â  Â  Â  Â  } else {
Â  Â  Â  Â  Â  parts.push({ text: message.text });
Â  Â  Â  Â  }
Â  Â  Â  }
Â  Â  Â Â 
Â  Â  Â  if (parts.length > 0) {
Â  Â  Â  Â  const lastRole = history.length > 0 ? history[history.length - 1].role : undefined;
Â  Â  Â  Â  if (lastRole === message.role) {
Â  Â  Â  Â  Â  console.warn(`Skipping message with duplicate consecutive role: ${message.role}`);
Â  Â  Â  Â  Â  continue;
Â  Â  Â  Â  }
Â  Â  Â  Â  history.push({ role: message.role, parts });
Â  Â  Â  }
Â  Â  }
Â  Â Â 
Â  Â  console.log(`ğŸ“Š Context Management: Final context - ${history.length} messages, ${totalImages} images, ~${estimatedTokens} tokens`);
Â  Â  return history;
Â  }

Â  /**
Â  Â * Reset chat sessions (from geminiService)
Â  Â */
Â  resetChat(): void {
Â  Â  console.log("Resetting all chat sessions.");
Â  Â  for (const key in this.chatSessions) {
Â  Â  Â  delete this.chatSessions[key];
Â  Â  }
Â  }

Â  /**
Â  Â * Check if chat is active (from geminiService)
Â  Â */
Â  isChatActive(conversationId: string): boolean {
Â  Â  return !!this.chatSessions[conversationId];
Â  }

Â  /**
Â  Â * Rename chat session (from geminiService)
Â  Â */
Â  renameChatSession(oldId: string, newId: string): void {
Â  Â  if (this.chatSessions[oldId] && !this.chatSessions[newId]) {
Â  Â  Â  console.log(`Moving chat session context from '${oldId}' to '${newId}'.`);
Â  Â  Â  this.chatSessions[newId] = this.chatSessions[oldId];
Â  Â  Â  delete this.chatSessions[oldId];
Â  Â  } else if (this.chatSessions[oldId] && this.chatSessions[newId]) {
Â  Â  Â  console.warn(`Cannot rename chat session: destination '${newId}' already exists. Context will not be moved.`);
Â  Â  } else if (!this.chatSessions[oldId]) {
Â  Â  Â  console.warn(`Cannot rename chat session: source '${oldId}' does not exist.`);
Â  Â  }
Â  }

Â  /**
Â  Â * Generate initial pro hint (from geminiService)
Â  Â */
Â  async generateInitialProHint(
Â  Â  prompt: string,
Â  Â  images: Array<{ base64: string; mimeType: string; }> | null,
Â  Â  conversation: Conversation,
Â  Â  history: ChatMessage[],
Â  Â  onError: (error: string) => void,
Â  Â  signal: AbortSignal
Â  ): Promise<string | null> {
Â  Â  if (await this.checkCooldown()) {
Â  Â  Â  onError('AI service is on cooldown. Please try again later.');
Â  Â  Â  return null;
Â  Â  }

Â  Â  const parts: Part[] = [];
Â  Â  const hasImages = images && images.length > 0;
Â  Â  if (hasImages) {
Â  Â  Â  images.forEach(image => {
Â  Â  Â  Â  parts.push({ inlineData: { data: image.base64, mimeType: image.mimeType } });
Â  Â  Â  });
Â  Â  }
Â  Â  parts.push({ text: prompt });
Â  Â Â 
    const geminiHistory = await this.mapMessagesToGeminiContent(history);

Â  Â  try {
Â  Â  Â  const modelToUse: GeminiModel = 'gemini-2.5-flash';
Â  Â  Â Â 
Â  Â  Â  const systemInstruction = await this.getSystemInstruction(conversation, hasImages || false);
Â  Â  Â Â 
Â  Â  Â  this.ensureAIInitialized();
Â  Â  Â  if (!this.ai) {
Â  Â  Â  Â  throw new Error('AI service not available: No API key provided');
Â  Â  Â  }

Â  Â  Â  const generateContentPromise = this.ai.models.generateContent({
Â  Â  Â  Â  model: modelToUse,
Â  Â  Â  Â  contents: [...geminiHistory, { role: 'user', parts }],
Â  Â  Â  Â  config: {
Â  Â  Â  Â  Â  systemInstruction,
Â  Â  Â  Â  Â  tools: [{ googleSearch: {} }]
Â  Â  Â  Â  },
Â  Â  Â  });

Â  Â  Â  const abortPromise = new Promise<never>((_, reject) => {
Â  Â  Â  Â  if (signal.aborted) return reject(new DOMException('Aborted', 'AbortError'));
Â  Â  Â  Â  signal.addEventListener('abort', () => reject(new DOMException('Aborted', 'AbortError')), { once: true });
Â  Â  Â  });
Â  Â  Â Â 
Â  Â  Â  const response = await Promise.race([generateContentPromise, abortPromise]);
Â  Â  Â Â 
Â  Â  Â  if (signal.aborted) return null;

Â  Â  Â  this.handleSuccess();
Â  Â  Â  return response.text || '';

Â  Â  } catch (error) {
Â  Â  Â  if (error instanceof DOMException && error.name === 'AbortError') {
Â  Â  Â  Â  console.log("Initial pro hint generation was aborted.");
Â  Â  Â  } else {
Â  Â  Â  Â  this.handleError(error, onError);
Â  Â  Â  }
Â  Â  Â  return null;
Â  Â  }
Â  }

Â  /**
Â  Â * Generate insight with search (from geminiService)
Â  Â */
Â  async generateInsightWithSearch(
Â  Â  prompt: string,
Â  Â  model: 'flash' | 'pro' = 'flash',
Â  Â  signal?: AbortSignal
Â  ): Promise<string> {
Â  Â  // Determine which model to use based on the model parameter
Â  Â  const modelName = model === 'pro' ? 'gemini-2.5-pro' : 'gemini-2.5-flash';
Â  Â Â 
Â  Â  // For cost optimization, always use Flash unless explicitly requested Pro
Â  Â  const finalModel = model === 'pro' ? 'gemini-2.5-pro' : 'gemini-2.5-flash';
Â  Â Â 
Â  Â  console.log(`ğŸ” Generating insight with ${finalModel} model (requested: ${model})`);

Â  Â  try {
Â  Â  Â  this.ensureAIInitialized();
Â  Â  Â  if (!this.ai) {
Â  Â  Â  Â  throw new Error('AI service not available: No API key provided');
Â  Â  Â  }

Â  Â  Â  const generateContentPromise = this.ai.models.generateContent({
Â  Â  Â  Â  model: finalModel,
Â  Â  Â  Â  contents: prompt,
Â  Â  Â  Â  config: {
Â  Â  Â  Â  Â  tools: [{ googleSearch: {} }],
Â  Â  Â  Â  },
Â  Â  Â  });

Â  Â  Â  // Handle abort if signal is provided
Â  Â  Â  if (signal) {
Â  Â  Â  Â  const abortPromise = new Promise<never>((_, reject) => {
Â  Â  Â  Â  Â  if (signal.aborted) return reject(new DOMException('Aborted', 'AbortError'));
Â  Â  Â  Â  Â  signal.addEventListener('abort', () => reject(new DOMException('Aborted', 'AbortError')), { once: true });
Â  Â  Â  Â  });

Â  Â  Â  Â  const response = await Promise.race([generateContentPromise, abortPromise]);
Â  Â  Â  Â  if (signal.aborted) throw new DOMException('Aborted', 'AbortError');
Â  Â  Â  Â Â 
Â  Â  Â  Â  this.handleSuccess();
Â  Â  Â  Â Â 
Â  Â  Â  Â  // Track API cost
Â  Â  Â  Â  apiCostService.recordAPICall(
Â  Â  Â  Â  Â  model,
Â  Â  Â  Â  Â  'user_query', // Default purpose, can be overridden
Â  Â  Â  Â  Â  'paid', // Default tier, can be overridden
Â  Â  Â  Â  Â  1000, // Default token estimate
Â  Â  Â  Â  Â  true
Â  Â  Â  Â  ).catch(error => console.error('Error tracking API cost:', error));
Â  Â  Â  Â Â 
Â  Â  Â  Â  return response.text || '';
Â  Â  Â  } else {
Â  Â  Â  Â  // No signal provided, just generate content
Â  Â  Â  Â  const response = await generateContentPromise;
Â  Â  Â  Â  this.handleSuccess();
Â  Â  Â  Â Â 
Â  Â  Â  Â  // Track API cost
Â  Â  Â  Â  apiCostService.recordAPICall(
Â  Â  Â  Â  Â  model,
Â  Â  Â  Â  Â  'user_query', // Default purpose, can be overridden
Â  Â  Â  Â  Â  'paid', // Default tier, can be overridden
Â  Â  Â  Â  Â  1000, // Default token estimate
Â  Â  Â  Â  Â  true
Â  Â  Â  Â  ).catch(error => console.error('Error tracking API cost:', error));
Â  Â  Â  Â Â 
Â  Â  Â  Â  return response.text || '';
Â  Â  Â  }

Â  Â  } catch (error) {
Â  Â  Â  if (error instanceof DOMException && error.name === 'AbortError') {
Â  Â  Â  Â  console.log(`Insight generation was aborted.`);
Â  Â  Â  Â  throw error;
Â  Â  Â  } else {
Â  Â  Â  Â  console.error(`Error in generateInsightWithSearch with ${finalModel}:`, error);
Â  Â  Â  Â  const errorMessage = error instanceof Error ? error.message : String(error);
Â  Â  Â  Â  // Re-throw a standardized error for the hook to catch
Â  Â  Â  Â  throw new Error(isQuotaError(error) ? "QUOTA_EXCEEDED" : errorMessage);
Â  Â  Â  }
Â  Â  }
Â  }

Â  /**
Â  Â * Generate insight stream (from geminiService)
Â  Â */
Â  async generateInsightStream(
Â  Â  gameName: string,
Â  Â  genre: string,
Â  Â  progress: number,
Â  Â  instruction: string,
Â  Â  insightId: string,
Â  Â  onChunk: (chunk: string) => void,
Â  Â  onError: (error: string) => void,
Â  Â  signal: AbortSignal
Â  ): Promise<void> {
Â  Â  if (await this.checkCooldown()) {
Â  Â  Â  onError('AI service is on cooldown. Please try again later.');
Â  Â  Â  return;
Â  Â  }

Â  Â  try {
Â  Â  Â  const model = this.getOptimalModel('insight_generation');
Â  Â  Â Â 
Â  Â  Â  const systemInstruction = this.getInsightSystemInstruction(gameName, genre, progress, instruction, insightId);
Â  Â  Â  const contentPrompt = `Generate the content for the "${insightId}" insight for the game ${gameName}, following the system instructions.`;
Â  Â  Â Â 
Â  Â  Â  // Check user tier to determine if grounding search should be enabled
Â  Â  Â  let tools: any[] = [];
Â  Â  Â  try {
Â  Â  Â  Â  const userTier = await unifiedUsageService.getTier();
Â  Â  Â  Â  if (userTier === 'pro' || userTier === 'vanguard_pro') {
Â  Â  Â  Â  Â  tools = [{ googleSearch: {} }];
Â  Â  Â  Â  Â  console.log(`ğŸ” Insight stream with grounding search for ${userTier} user`);
Â  Â  Â  Â  } else {
Â  Â  Â  Â  Â  tools = [];
Â  Â  Â  Â  Â  console.log(`ğŸš« Insight stream without grounding search for ${userTier} user`);
Â  Â  Â  Â  }
Â  Â  Â  } catch (error) {
Â  Â  Â  Â  console.warn('Failed to get user tier for insight stream, defaulting to no grounding search:', error);
Â  Â  Â  Â  tools = [];
Â  Â  Â  }
Â  Â  Â Â 
Â  Â  Â  this.ensureAIInitialized();
Â  Â  Â  if (!this.ai) {
Â  Â  Â  Â  throw new Error('AI service not available: No API key provided');
Â  Â  Â  }

Â  Â  Â  const streamPromise = this.ai.models.generateContentStream({
Â  Â  Â  Â  model,
Â  Â  Â  Â  contents: contentPrompt,
Â  Â  Â  Â  config: {Â 
Â  Â  Â  Â  Â  systemInstruction,
Â  Â  Â  Â  Â  tools
Â  Â  Â  Â  },
Â  Â  Â  });

Â  Â  Â  const abortPromise = new Promise<never>((_, reject) => {
Â  Â  Â  Â  if (signal.aborted) return reject(new DOMException('Aborted', 'AbortError'));
Â  Â  Â  Â  signal.addEventListener('abort', () => reject(new DOMException('Aborted', 'AbortError')), { once: true });
Â  Â  Â  });
Â  Â  Â Â 
Â  Â  Â  const stream = await Promise.race([streamPromise, abortPromise]);
Â  Â  Â Â 
Â  Â  Â  if (signal.aborted) return;
Â  Â  Â Â 
Â  Â  Â  await Promise.race([
Â  Â  Â  Â  (async () => {
Â  Â  Â  Â  Â  for await (const chunk of stream) {
Â  Â  Â  Â  Â  Â  if (signal.aborted) break;
Â  Â  Â  Â  Â  Â  if (chunk.text) {
Â  Â  Â  Â  Â  Â  Â  onChunk(chunk.text);
Â  Â  Â  Â  Â  Â  }
Â  Â  Â  Â  Â  }
Â  Â  Â  Â  })(),
Â  Â  Â  Â  abortPromise
Â  Â  Â  ]);

Â  Â  Â  this.handleSuccess();
Â  Â  } catch (error) {
Â  Â  Â  if (error instanceof DOMException && error.name === 'AbortError') {
Â  Â  Â  Â  console.log(`Insight generation for "${insightId}" was aborted.`);
Â  Â  Â  } else {
Â  Â  Â  Â  this.handleError(error, onError);
Â  Â  Â  }
Â  Â  }
  }

  override   cleanup(): void {
    console.log('ğŸ§¹ UnifiedAIService: Cleanup called');
    
    // âœ… MEMORY LEAK FIXES: Proper cleanup of all resources
    this.chatSessions = {};
    this.insightCache.clear();
    this.usedPrompts.clear();
    
    // Clear all intervals
    this.intervals.forEach(interval => clearInterval(interval));
    this.intervals.clear();
    
    // Abort all pending requests
    this.abortControllers.forEach(controller => controller.abort());
    this.abortControllers.clear();
    
    // Remove all event listeners
    this.eventListeners.forEach(cleanup => cleanup());
    this.eventListeners.clear();
  }
  
  // âœ… MEMORY LEAK FIXES: Track interval creation
  private createInterval(callback: () => void, delay: number): NodeJS.Timeout {
    const interval = setInterval(callback, delay);
    this.intervals.add(interval);
    return interval;
  }
  
  // âœ… MEMORY LEAK FIXES: Track abort controller creation
  private createAbortController(): AbortController {
    const controller = new AbortController();
    this.abortControllers.add(controller);
    return controller;
  }
  
  // âœ… MEMORY LEAK FIXES: Track event listener creation
  private addEventListener(element: EventTarget, event: string, handler: EventListener): void {
    element.addEventListener(event, handler);
    this.eventListeners.set(`${event}-${Date.now()}`, () => {
      element.removeEventListener(event, handler);
    });
  }
}

// Export singleton instance (lazy creation to avoid circular dependency issues)
let _unifiedAIService: UnifiedAIService | null = null;
export const unifiedAIService = (): UnifiedAIService => {
  if (!_unifiedAIService) {
    _unifiedAIService = new UnifiedAIService();
  }
  return _unifiedAIService;
};
